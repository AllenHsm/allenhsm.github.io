<?xml-stylesheet href="/rss.xsl" type="text/xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Allen&#39;s Blog</title>
    <link>http://allenhsm.github.io/</link>
    <description>Recent content on Allen&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This is a customized copyright.</copyright>
    <lastBuildDate>Mon, 01 Dec 2025 01:42:36 -0500</lastBuildDate>
    
        <atom:link href="http://allenhsm.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    
        <item>
        <title>Spark</title>
        <link>http://allenhsm.github.io/tech/cloud_infra_basics/spark/</link>
        <pubDate>Mon, 01 Dec 2025 01:42:36 -0500</pubDate>
        
        <guid>http://allenhsm.github.io/tech/cloud_infra_basics/spark/</guid>
        <description>Allen&#39;s Blog http://allenhsm.github.io/tech/cloud_infra_basics/spark/ -&lt;h1 id=&#34;why-spark&#34;&gt;&lt;strong&gt;Why Spark&lt;/strong&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Hadoop is not that efficient as it is designed for &lt;strong&gt;Commodity Hardware&lt;/strong&gt; while apache spark is designed for more expensive infra with better performance.&lt;/li&gt;
&lt;li&gt;Performance: MapReduce only supports
&lt;ul&gt;
&lt;li&gt;Full &lt;strong&gt;dump of intermediate data to disk&lt;/strong&gt; between jobs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No data sharing&lt;/strong&gt; between jobs
While Spark supports &lt;strong&gt;In-memory processing&lt;/strong&gt;,
benefits: &lt;figure&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/spark_inmemory.png&#34;
         alt=&#34;mapreduce vs spark&#34; width=&#34;65%&#34;/&gt;
&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MpReduce only supports &lt;strong&gt;batch&lt;/strong&gt; mode &lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/batch_vs_realtime.png&#34;
         alt=&#34;mapreduce vs spark&#34; width=&#34;65%&#34;/&gt;
&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;spark&#34;&gt;&lt;strong&gt;Spark&lt;/strong&gt;&lt;/h1&gt;
&lt;h2 id=&#34;basics&#34;&gt;&lt;strong&gt;Basics&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Apache Spark is a &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;general-purpose cluster computing system&lt;/strong&gt; for &lt;strong&gt;large scale data processing&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Spark was originally written in Scala, which allows &lt;strong&gt;concise function syntax&lt;/strong&gt; and interactive use.&lt;/li&gt;
&lt;li&gt;Scala is a &lt;strong&gt;type-safe Java Virtual Machine&lt;/strong&gt; language that incorporates both object oriented and functional programming into an extremely concise, logical, and extraordinarily powerful language.&lt;/li&gt;
&lt;li&gt;Apache Spark provides High-level APIs in Java, Scala, Python (PySpark) and R.&lt;/li&gt;
&lt;li&gt;Apache Spark combines two different modes of processing:
&lt;ul&gt;
&lt;li&gt;Batch-based Processing which could be provided via Apache Hadoop MapReduce&lt;/li&gt;
&lt;li&gt;Real-time Processing which could be provided via &lt;em&gt;&lt;strong&gt;Apache Storm&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Spark is run on CPU, and it &lt;strong&gt;can be used on GPU&lt;/strong&gt; using &lt;code&gt;Nvidia dgx spark&lt;/code&gt; with &lt;code&gt;RAPIDS&lt;/code&gt; accelerator.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ecosystem&#34;&gt;&lt;strong&gt;Ecosystem&lt;/strong&gt;&lt;/h2&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/spark_ecosystem.png&#34;
         alt=&#34;spark ecosystem&#34; width=&#34;50%&#34;/&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;engine&#34;&gt;&lt;strong&gt;Engine&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Lightning fast cluster computing&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Spark Core&lt;/strong&gt;&lt;/em&gt; is the general execution engine for the Spark platform that other functionalities are built on top of it. Spark has several advantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: Run programs up to &lt;strong&gt;100x faster than Hadoop&lt;/strong&gt; MapReduce in memory, or 10x faster on disk&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ease of Use&lt;/strong&gt;: Write applications quickly in Java, Scala, Python, R&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generality&lt;/strong&gt;: Combine SQL, streaming, and complex analytics&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Runs Everywhere&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Spark runs on Hadoop, Mesos, standalone, or in the cloud&lt;/li&gt;
&lt;li&gt;It can access diverse data sources including HDFS, Cassandra, HBase, and S3&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;why-fast&#34;&gt;&lt;strong&gt;Why Fast?&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Advanced DAG (Directed Acyclic Graph) execution engine&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It aims to rearrange and combine operators where possible to get the optimum performance&lt;/li&gt;
&lt;li&gt;i.e., Map-then-Filter will be rearranged to Filter-then-Map&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;In-memory distributed computing&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;It avoids disk writes and reads&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;It is achieved by the Resilient Distributed Datasets (RDDs)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;components&#34;&gt;&lt;strong&gt;Components&lt;/strong&gt;&lt;/h2&gt;
&lt;figure&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/spark_components.png&#34;
         alt=&#34;spark components&#34; width=&#34;55%&#34;/&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;resilient-distributed-datasets-rdds&#34;&gt;&lt;strong&gt;Resilient Distributed Datasets&lt;/strong&gt; (RDDs)&lt;/h2&gt;
&lt;h3 id=&#34;definition&#34;&gt;&lt;strong&gt;Definition&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;A distributed collection of objects across a cluster with user- controlled partitioning &amp;amp; storage&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;The Spark’s &lt;strong&gt;core abstraction&lt;/strong&gt; for working with data&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Resiliency&lt;/strong&gt;&lt;/em&gt;: capable of automatically rebuilding on failure
&lt;ul&gt;
&lt;li&gt;Each RDD contains lineage information (how it is built)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;properties&#34;&gt;&lt;strong&gt;Properties&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;RDDs shard the data over a cluster, like a virtualized, distributed collection (analogous to HDFS)&lt;/li&gt;
&lt;li&gt;RDDs support &lt;em&gt;&lt;strong&gt;intelligent caching&lt;/strong&gt;&lt;/em&gt;, which means no naive flushes of massive datasets to disk.
&lt;ul&gt;
&lt;li&gt;This feature alone allows Spark jobs to run 10-100x faster than comparable MapReduce jobs!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;“resilient”&lt;/strong&gt; part means they will reconstitute shards lost due to process/server crashes.&lt;/li&gt;
&lt;li&gt;RDD is &lt;strong&gt;partitioned across multiple nodes&lt;/strong&gt;. So, while the use of Spark will abstract away a lot of this partitioning from us, we need to know that RDDs will, in fact, &lt;strong&gt;be stored across the nodes&lt;/strong&gt; in the Spark cluster.&lt;/li&gt;
&lt;li&gt;Spark RDDs are &lt;em&gt;&lt;strong&gt;immutable&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Spark RDDs are resilient, that is, even if one of the nodes on which the RDD resides crashes, then it can be reconstructed using its metadata.&lt;/li&gt;
&lt;li&gt;Any operation which we specify on a Spark RDD will &lt;strong&gt;generate a new RDD&lt;/strong&gt; with those modifications applied. So RDDs cannot be modified or updated without the operations resulting in the creation of new RDDs with updated data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;programming&#34;&gt;&lt;strong&gt;Programming&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Lambda function: anonymous + lambda x avoids null or illegal input&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Example 1: Spark WordCount:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; sys
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pyspark &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; SparkContext, SparkConf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    conf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; SparkConf()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# create Spark context with necessary configuration&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    sc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; SparkContext&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;getOrCreate(conf&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;conf)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Conduct MapReduce and write the output to folder&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    wordCounts &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;textFile(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/testData&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;flatMap(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; line: line&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;))\
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; word: (word, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reduceByKey(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; a,b: a &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;saveAsTextFile(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/output&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Example 2: count the most frequent words:
&lt;ul&gt;
&lt;li&gt;first turn the word list into key-value with each value assigned to 1&lt;/li&gt;
&lt;li&gt;reduce the key-value pairs such that count the appearances: reduceByKey&lt;/li&gt;
&lt;li&gt;flip the relationship to value-key pair → sortByKey&lt;/li&gt;
&lt;li&gt;overall:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;results_rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; lines_rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;flatMap(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split())&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: (x,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                        &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reduceByKey(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x,y: x&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;y)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                        &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: (x[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;],x[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]))&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sortByKey(&lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;flexible-deployment&#34;&gt;&lt;strong&gt;Flexible Deployment&lt;/strong&gt;&lt;/h2&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/spark_deployment.png&#34;
         alt=&#34;spark context&#34; width=&#34;35%&#34;/&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;spark-context&#34;&gt;&lt;strong&gt;Spark Context&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;SparkContext&lt;/strong&gt; represents a Spark cluster’s connection that is useful in building &lt;strong&gt;RDDs&lt;/strong&gt; and broadcast variables on the cluster.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It enables your Spark Application to connect to the Spark Cluster using Resource Manager.&lt;/li&gt;
&lt;li&gt;Before the creation of SparkContext, SparkConf must be created&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In earlier versions of Apache Spark 1.x, Spark RDDs were the major APIs. However, Spark RDDs require a lot of low-level coding and you need much manual coding.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As a result, later versions were released to encapsulate Spark
RDDs into higher-level APIs, called &lt;strong&gt;Spark Dataframes&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;spark-dataframes&#34;&gt;&lt;strong&gt;Spark Dataframes&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;A history:
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/spark_history.png&#34;
         alt=&#34;spark dataframe history&#34; width=&#34;50%&#34;/&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Advantages of Spark Dataframes over RDDs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;At the minimum level, Spark still uses RDDs, but Dataframe API allows a &lt;strong&gt;much simpler interface&lt;/strong&gt; to interact with the data, as well as providing a more natural look and feel, especially with people familiar with relational databases.&lt;/li&gt;
&lt;li&gt;Spark Dataframes are like distributed in-memory tables with named columns and schemas, where each column has a specific data type: integer, string, array, map, real, date, timestamp, etc. To a human’s eye, a Spark DataFrame is &lt;em&gt;&lt;strong&gt;like a table&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Dataframes can be &lt;strong&gt;constructed from a wide array of sources&lt;/strong&gt;, such as structured data files, tables in Hive, external databases or existing RDDs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;spark-2x-entry-point&#34;&gt;Spark 2.x Entry Point&lt;/h3&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/spark_session.png&#34;
         alt=&#34;spark session&#34; width=&#34;40%&#34;/&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;architecture&#34;&gt;&lt;strong&gt;Architecture&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/spark_architecture.png&#34;
         alt=&#34;spark architecture&#34; width=&#34;43%&#34;/&gt;
&lt;/figure&gt;

The overall architecture for Spark is a typical &lt;em&gt;&lt;strong&gt;master/worker&lt;/strong&gt;&lt;/em&gt; configuration, where you have a master which coordinates the work across several workers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, there is a &lt;em&gt;&lt;strong&gt;driver&lt;/strong&gt;&lt;/em&gt;, which is an application that uses the &lt;em&gt;&lt;strong&gt;SparkContext&lt;/strong&gt;&lt;/em&gt;.
&lt;ul&gt;
&lt;li&gt;In Spark 2.x, &lt;em&gt;&lt;strong&gt;SparkSession&lt;/strong&gt;&lt;/em&gt; was created to &lt;strong&gt;encapsulate&lt;/strong&gt; SparkContext functionalities&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;SparkContext&lt;/code&gt; then refers to a &lt;em&gt;&lt;strong&gt;Cluster Manager&lt;/strong&gt;&lt;/em&gt; to interact with the various Workers.&lt;/li&gt;
&lt;li&gt;Each Worker consists of &lt;strong&gt;one Executor and one or more Tasks&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;It is the responsibility of the driver program to communicate&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;run-a-job&#34;&gt;&lt;strong&gt;Run a Job&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/spark_job.png&#34;
         alt=&#34;spark run job&#34; width=&#34;60%&#34;/&gt;
&lt;/figure&gt;

Basic code:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pyspark
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pyspark &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; SparkContext, SparkConf, SQLContext
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pyspark.sql &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; SparkSession
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;spark &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; SparkSession&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;builder \
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;appName(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;HW7_spark_session&amp;#34;&lt;/span&gt;) \
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;getOrCreate()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Spark application name:&amp;#34;&lt;/span&gt;, spark&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sparkContext&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;appName)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Master: &amp;#34;&lt;/span&gt;, spark&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sparkContext&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;master)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;dataframe-operations&#34;&gt;&lt;strong&gt;DataFrame Operations&lt;/strong&gt;&lt;/h1&gt;
&lt;h2 id=&#34;readwrite&#34;&gt;&lt;strong&gt;Read/Write&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The basic of reading data in Spark is through &lt;code&gt;DataFrameReader&lt;/code&gt;. This can be accessed through SparkSession through the read attribute&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pyspark &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; SparkFiles
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dataframe &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; spark&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;csv(SparkFiles&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;data.csv&amp;#34;&lt;/span&gt;), header&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;, inferSchema&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Or simply&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dataframe &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; spark&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;csv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;data.csv&amp;#34;&lt;/span&gt;, header&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;, inferSchema&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;dataframe-schema&#34;&gt;&lt;strong&gt;DataFrame Schema&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Think about each dataframe as a table. The table &lt;strong&gt;MUST&lt;/strong&gt; have a structure (e.g. column names and data types). In Spark, this structure is called &lt;strong&gt;Schema&lt;/strong&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/spark_schema.png&#34;
         alt=&#34;spark dataframe schema&#34; width=&#34;45%&#34;/&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;data-types&#34;&gt;&lt;strong&gt;Data Types&lt;/strong&gt;&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Data type&lt;/th&gt;
&lt;th&gt;Value assigned in Python&lt;/th&gt;
&lt;th&gt;API to instantiate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ByteType&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;DataTypes.ByteType&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ShortType&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;DataTypes.ShortType&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IntegerType&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;DataTypes.IntegerType&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LongType&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;DataTypes.LongType&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FloatType&lt;/td&gt;
&lt;td&gt;float&lt;/td&gt;
&lt;td&gt;DataTypes.FloatType&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DoubleType&lt;/td&gt;
&lt;td&gt;float&lt;/td&gt;
&lt;td&gt;DataTypes.DoubleType&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;StringType&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;DataTypes.StringType&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BooleanType&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;DataTypes.BooleanType&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DecimalType&lt;/td&gt;
&lt;td&gt;decimal.Decimal&lt;/td&gt;
&lt;td&gt;DecimalType&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;more-operations&#34;&gt;&lt;strong&gt;More Operations&lt;/strong&gt;&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print (dataframe&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;columns)  &lt;span style=&#34;color:#75715e&#34;&gt;# get column names&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(dataframe&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;count())  &lt;span style=&#34;color:#75715e&#34;&gt;# get number of rows&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dataframe&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, vertical&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;true)  &lt;span style=&#34;color:#75715e&#34;&gt;# show first 5 rows&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dataframe&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;describe()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()  &lt;span style=&#34;color:#75715e&#34;&gt;# get summary statistics, will print count, mean, stddev, min, max for numeric columns&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;spark-sql&#34;&gt;&lt;strong&gt;Spark SQL&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;Spark SQL&lt;/code&gt; in Spark Ecosystem:
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/spark_sql.png&#34;
         alt=&#34;spark sql&#34; width=&#34;40%&#34;/&gt;
&lt;/figure&gt;

Once you have read-in your dataframe, you may want to conduct some operations on it. Most of these operations belong to the &lt;strong&gt;Spark SQL&lt;/strong&gt; library.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spark SQL is part of the core distribution since Spark 1.0 (April 2014)&lt;/li&gt;
&lt;li&gt;Spark SQL runs SQL and HiveQL queries together.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sql-dataframe-operations&#34;&gt;&lt;strong&gt;SQL DataFrame Operations&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Relational operations (e.g., select, where, join, groupBy) via a Domain
Specific Language&lt;/li&gt;
&lt;li&gt;Operators take &lt;em&gt;&lt;strong&gt;expression&lt;/strong&gt;&lt;/em&gt; objects&lt;/li&gt;
&lt;li&gt;Operators build up an Abstract Syntax Tree (AST), which is then
optimized by &lt;em&gt;&lt;strong&gt;Catalyst&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;e.g.
&lt;ul&gt;
&lt;li&gt;display number of players with first name as &amp;ldquo;John&amp;rdquo;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pyspark.sql.functions &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; col
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dataframe&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;where(col(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;first_name&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;John&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;count()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;Create a Subset Dataframe from your Dataframe
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;subset_df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dataframe&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;select(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;first_name&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;last_name&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;age&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;subset_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;Display unique values from a column in your dataframe
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dataframe&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;select(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;LastName&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;distinct()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;Filter records in your dataframe
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pyspark.sql.functions &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; col
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dataframe&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;select(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;nflId&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;FirstName&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;LastName&amp;#34;&lt;/span&gt;) \
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;where(col(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;FirstName&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;John&amp;#34;&lt;/span&gt;))&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;Convert SparkDataFrame into RDD and Vice Versa:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dataframe&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rdd
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;updated_df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;toDF()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How does &lt;em&gt;Catalyst&lt;/em&gt; work?
&lt;figure&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/spark_catalyst.png&#34;
           alt=&#34;spark catalyst optimizer&#34; width=&#34;50%&#34;/&gt;
  &lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;advantages-over-relational-query-languages&#34;&gt;&lt;strong&gt;Advantages over Relational Query Languages&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Holistic optimization across functions composed in different languages.&lt;/li&gt;
&lt;li&gt;Control structures (e.g., if, for)&lt;/li&gt;
&lt;li&gt;Logical plan analyzed eagerly → identify code errors associated with data &lt;em&gt;&lt;strong&gt;schema&lt;/strong&gt;&lt;/em&gt; issues on the fly.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;rdd-operations&#34;&gt;&lt;strong&gt;RDD Operations&lt;/strong&gt;&lt;/h1&gt;
&lt;h2 id=&#34;transformations-vs-actions&#34;&gt;&lt;strong&gt;Transformations vs Actions&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&#34;transformations&#34;&gt;&lt;strong&gt;Transformations&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Transformations&lt;/strong&gt; are where the Spark machinery can do its magic with lazy evaluation and clever algorithms to minimize communication and parallelize the processing.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generate a new RDD from existing one&lt;/li&gt;
&lt;li&gt;Lazy/delay evaluation (not until an action performs): e.g. several transformations will be sent to catalyst optimizer, and it will not execute until an &lt;strong&gt;action&lt;/strong&gt;. &lt;strong&gt;Actions&lt;/strong&gt; are eagerly evaluated&lt;/li&gt;
&lt;li&gt;If a job is to create another RDD from the input data, it is a transformation&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;actions&#34;&gt;&lt;strong&gt;Actions&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Actions are mostly used either &lt;em&gt;&lt;strong&gt;at the end of the analysis&lt;/strong&gt;&lt;/em&gt; when the data have been distilled down (collect), or &lt;em&gt;&lt;strong&gt;along the way to &amp;ldquo;peek&amp;rdquo; at the process&lt;/strong&gt;&lt;/em&gt; (count, take).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Trigger a computation on RDD and do something with the results either
returning them to the user (driver) or saving them to external storage&lt;/li&gt;
&lt;li&gt;Have immediate effect&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;programming-1&#34;&gt;&lt;strong&gt;Programming&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&#34;single-rdd&#34;&gt;&lt;strong&gt;Single RDD&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Transformations&lt;/strong&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Transformation&lt;/th&gt;
&lt;th&gt;Result&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;map&lt;/strong&gt;&lt;/em&gt;(func)&lt;/td&gt;
&lt;td&gt;Return a new RDD by passing each element through &lt;em&gt;func&lt;/em&gt;. (Same Size)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;filter&lt;/strong&gt;&lt;/em&gt;(func)&lt;/td&gt;
&lt;td&gt;Return a new RDD by selecting the elements for which &lt;em&gt;func&lt;/em&gt; returns true. (Fewer Elements)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;flatMap&lt;/strong&gt;&lt;/em&gt;(func)&lt;/td&gt;
&lt;td&gt;&lt;em&gt;func&lt;/em&gt; can return multiple items and generate a sequence, allowing flattening nested entries into a list. (More Elements)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;distinct&lt;/strong&gt;&lt;/em&gt;()&lt;/td&gt;
&lt;td&gt;Return an RDD with only distinct entries.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;sample&lt;/strong&gt;&lt;/em&gt;(&amp;hellip;)&lt;/td&gt;
&lt;td&gt;Various options to create a subset of the RDD.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;union&lt;/strong&gt;&lt;/em&gt;(RDD)&lt;/td&gt;
&lt;td&gt;Return a union of the RDDs.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;intersection&lt;/strong&gt;&lt;/em&gt;(RDD)&lt;/td&gt;
&lt;td&gt;Return an intersection of the RDDs.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;subtract&lt;/strong&gt;&lt;/em&gt;(RDD)&lt;/td&gt;
&lt;td&gt;Remove argument RDD from other.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;cartesian&lt;/strong&gt;&lt;/em&gt;(RDD)&lt;/td&gt;
&lt;td&gt;Cartesian product of the RDDs.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;parallelize&lt;/strong&gt;&lt;/em&gt;(list)&lt;/td&gt;
&lt;td&gt;Create an RDD from this (Python) list (using a spark context).&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Actions&lt;/strong&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Action&lt;/th&gt;
&lt;th&gt;Result&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;collect&lt;/strong&gt;&lt;/em&gt;()&lt;/td&gt;
&lt;td&gt;Return all the elements from the RDD.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;count&lt;/strong&gt;&lt;/em&gt;()&lt;/td&gt;
&lt;td&gt;Number of elements in RDD.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;countByValue&lt;/strong&gt;&lt;/em&gt;()&lt;/td&gt;
&lt;td&gt;List of times each value occurs in the RDD.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;reduce&lt;/strong&gt;&lt;/em&gt;(func)&lt;/td&gt;
&lt;td&gt;Aggregate the elements of the RDD by providing a function which combines any two into one (sum, min, max, …).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;first&lt;/strong&gt;&lt;/em&gt;(), &lt;em&gt;&lt;strong&gt;take&lt;/strong&gt;&lt;/em&gt;(n)&lt;/td&gt;
&lt;td&gt;Return the first, or first n elements.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;top&lt;/strong&gt;&lt;/em&gt;(n)&lt;/td&gt;
&lt;td&gt;Return the n highest valued elements of the RDDs.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;takeSample&lt;/strong&gt;&lt;/em&gt;(&amp;hellip;)&lt;/td&gt;
&lt;td&gt;Various options to return a subset of the RDD.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;saveAsTextFile&lt;/strong&gt;&lt;/em&gt;(path)&lt;/td&gt;
&lt;td&gt;Write the elements as a text file.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;foreach&lt;/strong&gt;&lt;/em&gt;(func)&lt;/td&gt;
&lt;td&gt;Run the &lt;em&gt;func&lt;/em&gt; on each element. Used for side-effects (updating accumulator variables) or interacting with external systems.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;pair-rdd&#34;&gt;&lt;strong&gt;Pair RDD&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Key/Value organization is a simple, but often requires very efficient schema.&lt;/p&gt;
&lt;p&gt;Spark provides special operations on RDDs that contain key/value pairs.&lt;/p&gt;
&lt;p&gt;On the language (Python, Scala, Java) side, key/values are simply tuples. If you have an RDD all of whose elements happen to be tuples of two items, it is a Pair RDD and you can use the key/value operations that follow.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transformation&lt;/strong&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Transformation&lt;/th&gt;
&lt;th&gt;Result&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;reduceByKey&lt;/strong&gt;&lt;/em&gt;(func)&lt;/td&gt;
&lt;td&gt;Reduce values using &lt;em&gt;func&lt;/em&gt;, but on a key-by-key basis. That is, combine values with the same key.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;groupByKey&lt;/strong&gt;&lt;/em&gt;()&lt;/td&gt;
&lt;td&gt;Combine values with same key. Each key ends up with a list.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;sortByKey&lt;/strong&gt;&lt;/em&gt;()&lt;/td&gt;
&lt;td&gt;Return an RDD sorted by key.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;mapValues&lt;/strong&gt;&lt;/em&gt;(func)&lt;/td&gt;
&lt;td&gt;Use &lt;em&gt;func&lt;/em&gt; to change values, but not key.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;keys&lt;/strong&gt;&lt;/em&gt;()&lt;/td&gt;
&lt;td&gt;Return an RDD of only keys.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;values&lt;/strong&gt;&lt;/em&gt;()&lt;/td&gt;
&lt;td&gt;Return an RDD of only values.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Action&lt;/strong&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Action&lt;/th&gt;
&lt;th&gt;Result&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;countByKey&lt;/strong&gt;&lt;/em&gt;()&lt;/td&gt;
&lt;td&gt;Count the number of elements for each key.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;lookup&lt;/strong&gt;&lt;/em&gt;(key)&lt;/td&gt;
&lt;td&gt;Return all the values for this key.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;two-pair&#34;&gt;&lt;strong&gt;Two Pair&lt;/strong&gt;&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Transformation&lt;/th&gt;
&lt;th&gt;Result&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;subtractByKey&lt;/strong&gt;&lt;/em&gt;(otherRDD)&lt;/td&gt;
&lt;td&gt;Remove elements with a key present in other RDD.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;join&lt;/strong&gt;&lt;/em&gt;(otherRDD)&lt;/td&gt;
&lt;td&gt;Inner join: Return an RDD containing all pairs of elements with matching keys in self and other. Each pair of elements will be returned as a &lt;code&gt;(k, (v1, v2))&lt;/code&gt; tuple, where &lt;code&gt;(k, v1)&lt;/code&gt; is in self and &lt;code&gt;(k, v2)&lt;/code&gt; is in other.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;leftOuterJoin&lt;/strong&gt;&lt;/em&gt;(otherRDD)&lt;/td&gt;
&lt;td&gt;For each element &lt;code&gt;(k, v)&lt;/code&gt; in self, the resulting RDD will either contain all pairs &lt;code&gt;(k, (v, w))&lt;/code&gt; for w in other, or the pair &lt;code&gt;(k, (v, None))&lt;/code&gt; if no elements in other have key k.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;rightOuterJoin&lt;/strong&gt;&lt;/em&gt;(otherRDD)&lt;/td&gt;
&lt;td&gt;For each element (k, w) in other, the resulting RDD will either contain all pairs &lt;code&gt;(k, (v, w))&lt;/code&gt; for v in this, or the pair (k, (None, w)) if no elements in self have key k.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;cogroup&lt;/strong&gt;&lt;/em&gt;(otherRDD)&lt;/td&gt;
&lt;td&gt;Group data from both RDDs by key.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;top-n-example&#34;&gt;&lt;strong&gt;Top-N Example&lt;/strong&gt;&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;lines_rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; scl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;textFile(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;inputData.txt&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Count the number of lines&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;lines_rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;count()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Number of words&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;words_rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; lines_rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;flatMap(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;words_rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;count()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Number of distinct words&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;words_rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;distinct()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;count()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Count the occurrences of each word &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;key_value_rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; words_rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: (x, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;word_count_rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; key_value_rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reduceByKey(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x, y: x &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; y)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# find top-n frequent words&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;flipped_rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; word_count_rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: (x[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], x[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sorted_rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; flipped_rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sortByKey(&lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Combine all in one&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;results_rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; lines_rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;flatMap(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split())&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: (x,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                        &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reduceByKey(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x,y: x&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;y)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                        &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: (x[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;],x[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]))&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sortByKey(&lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;extra-reading&#34;&gt;&lt;strong&gt;Extra Reading&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://spark.apache.org/docs/2.3.0/sql-programming-guide.html&#34;&gt;Spark SQL, Dataframe and Dataset Guide&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://spark.apache.org/docs/latest/sql-performance-tuning.html&#34;&gt;Spark Optimizations&lt;/a&gt;&lt;/p&gt;
- http://allenhsm.github.io/tech/cloud_infra_basics/spark/ - This is a customized copyright.</description>
        </item>
    
    
    
        <item>
        <title>Hadoop - YARN</title>
        <link>http://allenhsm.github.io/tech/cloud_infra_basics/hadoop_yarn/</link>
        <pubDate>Sat, 29 Nov 2025 04:56:00 -0500</pubDate>
        
        <guid>http://allenhsm.github.io/tech/cloud_infra_basics/hadoop_yarn/</guid>
        <description>Allen&#39;s Blog http://allenhsm.github.io/tech/cloud_infra_basics/hadoop_yarn/ -&lt;h1 id=&#34;yarn&#34;&gt;Yarn&lt;/h1&gt;
&lt;h2 id=&#34;brief-history&#34;&gt;Brief History&lt;/h2&gt;
&lt;h3 id=&#34;hadoop-1x&#34;&gt;Hadoop 1.x&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Core components: &lt;strong&gt;MapReduce&lt;/strong&gt; and &lt;strong&gt;HDFS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Structure:
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;HDFS:
  NameNode + DataNodes

MapReduce (MRv1):
  JobTracker + TaskTrackers
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;JobTracker&lt;/strong&gt;&lt;/em&gt; did everything: resource management, job scheduling, tracking tasks, failure handling
&lt;ul&gt;
&lt;li&gt;Also a &lt;strong&gt;single point of failure&lt;/strong&gt; and a scaling bottleneck&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;MapReduce&lt;/strong&gt;&lt;/em&gt; = computation engine + resource manager in one&lt;/li&gt;
&lt;li&gt;Only &lt;code&gt;MapReduce&lt;/code&gt; jobs were supported&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hadoop-2x&#34;&gt;Hadoop 2.x&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Now &lt;code&gt;Hadoop&lt;/code&gt; has three logical layers:
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;HDFS: storage layer
  NameNode + DataNodes

YARN: resource management layer
  ResourceManager + NodeManagers + ApplicationMasters

MapReduce (MRv2): computation layer
  Runs on top of YARN as one of many application frameworks
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;Key &lt;strong&gt;improvements&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Separated resource management from computation&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;MapReduce no longer monopolizes the cluster&lt;/li&gt;
&lt;li&gt;Allowed more engines: Spark, Tez, Flink, Storm, etc.&lt;/li&gt;
&lt;li&gt;Better scalability and cluster utilization&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hadoop-3x&#34;&gt;Hadoop 3.x&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Structure:
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Namespace Federation:
    Multiple NameNodes managing independent namespaces
    Shared pool of DataNodes

YARN:
    Handles multi-framework distributed scheduling
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;Improvements:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Federation &amp;amp; NameNode high availability&lt;/strong&gt;&lt;/em&gt; improvements&lt;/li&gt;
&lt;li&gt;Erasure coding (reduces replication overhead)&lt;/li&gt;
&lt;li&gt;More NameNodes with shared DataNodes (HDFS federation)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Yet Another Resource Negotiator&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;It is the &lt;strong&gt;orchestrator&lt;/strong&gt; of the Hadoop MapReduce jobs starting from Hadoop 2.x.&lt;/li&gt;
&lt;li&gt;As the amount of data increases and 4k nodes are not enough, starting from 2.x. the two main components of Hadoop changed from MapReduce and HDFS to Yarn and HDFS.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;daemons&#34;&gt;Daemons&lt;/h2&gt;
&lt;p&gt;: provides resource management&lt;/p&gt;
&lt;h3 id=&#34;two-long-running&#34;&gt;Two Long Running&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Resource manager&lt;/strong&gt;&lt;/em&gt; - 1 / cluster
&lt;ul&gt;
&lt;li&gt;Manages resource and schedules across the cluster, especially large clusters&lt;/li&gt;
&lt;li&gt;runs on Master node&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Node managers&lt;/strong&gt;&lt;/em&gt; - 1 / node, similar to &lt;code&gt;kubectl&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;launches and monitors job containers&lt;/li&gt;
&lt;li&gt;runs on worker nodes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;dynamic-components&#34;&gt;Dynamic Components&lt;/h2&gt;
&lt;p&gt;(They are only created when a job have to run)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Daemons create two main types of Yarn components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Containers&lt;/strong&gt;&lt;/em&gt; (to host tasks)
&lt;ul&gt;
&lt;li&gt;Created by the RM upon request&lt;/li&gt;
&lt;li&gt;Runs with a certain amount of resources (memory, CPU) on a worker
node&lt;/li&gt;
&lt;li&gt;An application can run as one or more tasks in one or more containers.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;MapReduce Application Master&lt;/strong&gt;&lt;/em&gt; (MRAppMaster)
&lt;ul&gt;
&lt;li&gt;One per application&lt;/li&gt;
&lt;li&gt;Framework/application specific&lt;/li&gt;
&lt;li&gt;Requests more containers to run application tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;anatomy-of-mapreduce-job&#34;&gt;Anatomy of MapReduce job&lt;/h2&gt;
&lt;p&gt;5 Phases:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Job Submission&lt;/li&gt;
&lt;li&gt;MRAppMaster Initialization&lt;/li&gt;
&lt;li&gt;MapReduce Job Initialization&lt;/li&gt;
&lt;li&gt;MapReduce Task Execution&lt;/li&gt;
&lt;li&gt;Job Completion&lt;/li&gt;
&lt;/ol&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/mapreduce_anatomy.png&#34;
         alt=&#34;yarn mapreduce anatomy&#34; width=&#34;80%&#34;/&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;job-submission&#34;&gt;&lt;strong&gt;Job submission&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;step 1-4&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When you run MapReduce Job, it creates an internal JobSubmitter instance and calls &lt;code&gt;submitJobInternal()&lt;/code&gt;. This function does the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;submit python code, a library convert it to a java file&lt;/li&gt;
&lt;li&gt;Asks the RM for a new &lt;code&gt;**App ID**&lt;/code&gt;, used as MapReduce Job ID (&lt;strong&gt;step 2&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Checks the job configurations, i.e., output, input paths, etc before it copies anything (like if there existing the same output folder) as a part of job submission&lt;/li&gt;
&lt;li&gt;Compute &lt;strong&gt;input splits&lt;/strong&gt; from block information returned from RM&lt;/li&gt;
&lt;li&gt;Copies the resources needed to run the job to the &lt;strong&gt;shared file system&lt;/strong&gt; (&lt;strong&gt;step 3)&lt;/strong&gt;
&lt;ol&gt;
&lt;li&gt;Job jar file (or source files), configuration file, &lt;strong&gt;input splits&lt;/strong&gt;(when the input is small)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Submits the job by calling &lt;code&gt;submitApplication()&lt;/code&gt; on the RM (&lt;strong&gt;step 4&lt;/strong&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;mrappmaster-initialization&#34;&gt;&lt;strong&gt;MRAppMaster Initialization&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Step 5a, 5b&lt;/li&gt;
&lt;li&gt;RM receives a call to &lt;code&gt;submitApplication()&lt;/code&gt; and hands off the request to the &lt;strong&gt;YARN scheduler&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The scheduler allocates a &lt;strong&gt;container&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;scheduler is very parallel and close to the resource manager and people sometimes consider them as the same thing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The RM launches the &lt;strong&gt;MRAppMaster&lt;/strong&gt; Process through the Node Manager (&lt;em&gt;&lt;strong&gt;step 5a, b&lt;/strong&gt;&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;mapreduce-job-initialization&#34;&gt;&lt;strong&gt;MapReduce Job Initialization&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;by AppMaster: Step 6, 7, 8&lt;/li&gt;
&lt;li&gt;MRAppMaster initializes the MapReduce job by creating bookkeeping objects to keep track of the job’s progress (&lt;em&gt;&lt;strong&gt;step 6&lt;/strong&gt;&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Retrieves the &lt;strong&gt;input splits&lt;/strong&gt; (not copying, but retrieving their locations) computed in the client from the shared file system (&lt;em&gt;&lt;strong&gt;step 7&lt;/strong&gt;&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Decides how to run the tasks&lt;/li&gt;
&lt;li&gt;Example of resource request:
&lt;ul&gt;
&lt;li&gt;Resource name (hostname, rack)&lt;/li&gt;
&lt;li&gt;Priority (within this application, usually map task is higher)&lt;/li&gt;
&lt;li&gt;Resource requirements: memory, CPU, etc&lt;/li&gt;
&lt;li&gt;Number of containers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Subtopics:
&lt;ul&gt;
&lt;li&gt;IF the tasks are small?
&lt;ul&gt;
&lt;li&gt;MRAppMaster will run the task in the same memory as itself - &lt;strong&gt;Uberization:&lt;/strong&gt; uber the data from HDFS to the memory as the file size is small.&lt;/li&gt;
&lt;li&gt;Because Overhead of allocating and running tasks in the new containers outweighs the parallel computation benefit&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Small job&lt;/strong&gt;:  less than 10 mappers, one reducer, and input size &amp;lt; the size of HDFS block&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If the tasks are big?
&lt;ul&gt;
&lt;li&gt;If the job is not &lt;strong&gt;uberized,&lt;/strong&gt; MRAppMAster requests containers for all the map and reduce tasks from the RM.&lt;/li&gt;
&lt;li&gt;Request specifies:
&lt;ul&gt;
&lt;li&gt;Priority (map &amp;gt; reduce)&lt;/li&gt;
&lt;li&gt;Memory (default 1GB)&lt;/li&gt;
&lt;li&gt;CPU (default 1 virtual core)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Map tasks are assigned based on &lt;strong&gt;data locality&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;mapreduce-task-execution&#34;&gt;&lt;strong&gt;MapReduce task execution&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Implemented for Each Task in the job(step 9ab - 10)&lt;/li&gt;
&lt;li&gt;Example of Launch context:
&lt;ul&gt;
&lt;li&gt;Container ID&lt;/li&gt;
&lt;li&gt;Commands (to start the app)&lt;/li&gt;
&lt;li&gt;Environment( configuration)&lt;/li&gt;
&lt;li&gt;Local Resources (app binary, HDFS files)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Once a request is fulfilled – a container on a particular node is assigned to a specific task. (&lt;em&gt;&lt;strong&gt;Step 9a&lt;/strong&gt;&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;MRAppMaster starts a MapReduce application whose main class is YarnChild in that task via the Node Manager (&lt;em&gt;&lt;strong&gt;Step 9b&lt;/strong&gt;&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Localizes the resources needed by the tasks including the job configuration and the source file(s) (&lt;em&gt;&lt;strong&gt;Step 10&lt;/strong&gt;&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Requests for reduce tasks are not made by RM &lt;strong&gt;until 5% completion&lt;/strong&gt; of map tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;mapreduce-job-completion&#34;&gt;&lt;strong&gt;MapReduce Job Completion&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Once the MRAppMaster receives a completion notification from the last task, it changes the job status to “successful”  (the _Success file in the output folder on GCP)&lt;/li&gt;
&lt;li&gt;Job polls for status and learns that the job has completed successfully&lt;/li&gt;
&lt;li&gt;MRAppMaster prints a message, some statistics, and counters to the console&lt;/li&gt;
&lt;li&gt;MRAppMaster and its tasks clean up their working state&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;edit-yarn-daemon-config&#34;&gt;Edit YARN Daemon Config&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Check the &lt;code&gt;hadoop.env&lt;/code&gt; file and look for YARN configurations starting from line number 11.&lt;/li&gt;
&lt;li&gt;Can change things like:
&lt;ul&gt;
&lt;li&gt;Your scheduling algorithm that defines the priority of execution for your job tasks. By default, it uses Capacity Scheduler.&lt;/li&gt;
&lt;li&gt;Whether your data should be compressed&lt;/li&gt;
&lt;li&gt;Memory allocated for your resource manager, your node manager, etc&lt;/li&gt;
&lt;li&gt;The minimum and maximum size of your input split by adding
&lt;code&gt;-D mapred.min.split.size&lt;/code&gt;
&lt;code&gt;-D mapred.max.split.size&lt;/code&gt;
(to control the number of mappers that will be created)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;spark-on-yarn-client-mode&#34;&gt;Spark on Yarn: Client mode&lt;/h2&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/yarn_with_spark.png&#34;
         alt=&#34;spark on yarn client mode&#34; width=&#34;60%&#34;/&gt;
&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;Because the driver runs on a client, so it requires:
&lt;ul&gt;
&lt;li&gt;interactive components like spark shell or pyspark&lt;/li&gt;
&lt;li&gt;good for debugging and testing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The driver runs on the cluster in the YARN application master
&lt;ul&gt;
&lt;li&gt;The entire of program runs in the cluster&lt;/li&gt;
&lt;li&gt;production jobs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reading&#34;&gt;Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Hadoop YARN Architecture: &lt;a href=&#34;https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html&#34;&gt;https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hadoop YARN Commands: &lt;a href=&#34;https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YarnCommands.html&#34;&gt;https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YarnCommands.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
- http://allenhsm.github.io/tech/cloud_infra_basics/hadoop_yarn/ - This is a customized copyright.</description>
        </item>
    
    
    
        <item>
        <title>Hadoop - MapReduce</title>
        <link>http://allenhsm.github.io/tech/cloud_infra_basics/hadoop_mapreduce/</link>
        <pubDate>Sat, 29 Nov 2025 04:55:54 -0500</pubDate>
        
        <guid>http://allenhsm.github.io/tech/cloud_infra_basics/hadoop_mapreduce/</guid>
        <description>Allen&#39;s Blog http://allenhsm.github.io/tech/cloud_infra_basics/hadoop_mapreduce/ -&lt;h1 id=&#34;intro&#34;&gt;Intro&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;How &lt;em&gt;traditional&lt;/em&gt; distributed system dealing with data:
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/traditional_DS.png&#34;
         alt=&#34;traditional distributed system&#34; width=&#34;60%&#34;/&gt;
&lt;/figure&gt;

It is too complicated to build a distributed system from scratch&lt;/li&gt;
&lt;li&gt;Now &lt;em&gt;&lt;strong&gt;MapReduce&lt;/strong&gt;&lt;/em&gt; comes in and abbreviates this process by taking care of most parts, except for collecting results, and processing the data before handling failures.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;mapreduce&#34;&gt;MapReduce&lt;/h1&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;programming model&lt;/strong&gt; and an &lt;strong&gt;associated implementation&lt;/strong&gt; for &lt;em&gt;&lt;strong&gt;processing and generating large data sets&lt;/strong&gt;&lt;/em&gt; with a &lt;em&gt;&lt;strong&gt;parallel, distributed algorithm&lt;/strong&gt;&lt;/em&gt; on a Hadoop cluster.&lt;/li&gt;
&lt;li&gt;It abstracts the problem from disk reads and writes, transforming it into a &lt;strong&gt;computation over sets for keys and values&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;It works as a &lt;em&gt;&lt;strong&gt;batch query processor&lt;/strong&gt;&lt;/em&gt; that can run an ad hoc query against your whole data set and get result in a reasonable time. Not real time processing.
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/mapreduce_intro.png&#34;
           alt=&#34;mapreduce overview&#34; width=&#34;65%&#34;/&gt;
  &lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;scalability-and-parallelism&#34;&gt;Scalability and Parallelism&lt;/h2&gt;
&lt;h3 id=&#34;word-count-example&#34;&gt;Word Count Example:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Input: a large collection of documents&lt;/li&gt;
&lt;li&gt;Output: a list of (word, count) pairs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First phase:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Define WordCount &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; Multiset;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  For Each Document &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; DocumentSubset {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      T &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tokenize(document);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      For Each Token &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; T {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          WordCount[token]&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      } 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  SendToSecondPhase(wordCount);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Second phase:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Define WordCount &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; Multiset;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;For Each Document &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; DocumentSubset {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    T &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tokenize(document);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    For Each Token &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; T {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        WordCount[token]&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    } }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;SendToSecondPhase(wordCount);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;We want to execute both phases in a &lt;em&gt;&lt;strong&gt;distributed manner&lt;/strong&gt;&lt;/em&gt; on a cluster of machines that can run independently.&lt;/li&gt;
&lt;li&gt;So we do Divide and Conquer:
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/mapreduce_d&amp;amp;c.png&#34;
         alt=&#34;mapreduce divide and conquer&#34; width=&#34;45%&#34;/&gt;
&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Challenges&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;How do we assign work units to workers?&lt;/li&gt;
&lt;li&gt;What if we have more work units than workers?&lt;/li&gt;
&lt;li&gt;What if workers need to share partial results?&lt;/li&gt;
&lt;li&gt;How do we aggregate partial results?&lt;/li&gt;
&lt;li&gt;How do we know all the workers have finished?&lt;/li&gt;
&lt;li&gt;What if workers fail?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Distributed system developer mindset:
&lt;ul&gt;
&lt;li&gt;The order in which workers run may be unknown&lt;/li&gt;
&lt;li&gt;The order in which workers interrupt each other may be unknown&lt;/li&gt;
&lt;li&gt;The order in which workers access shared data may be unknown&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;details-in-mapreduce&#34;&gt;Details in MapReduce&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;data intensive&lt;/strong&gt; programming model as a framework for &lt;em&gt;&lt;strong&gt;key-based operations&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Underlying runtime system (&lt;strong&gt;YARN + HDFS&lt;/strong&gt;)
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Automatically parallelizes&lt;/em&gt; the computation across large-scale clusters of machines&lt;/li&gt;
&lt;li&gt;Handles machine failures, communications and performance issues.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What are &lt;em&gt;&lt;strong&gt;Map&lt;/strong&gt;&lt;/em&gt; and &lt;em&gt;&lt;strong&gt;Reduce&lt;/strong&gt;&lt;/em&gt; functions:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Map&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Iterate over a large number of records&lt;/li&gt;
&lt;li&gt;Extract something of interest from each record&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Shuffle&lt;/strong&gt;: Shuffle and sort intermediate results - Taken care by &lt;code&gt;Hadoop&lt;/code&gt;!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reduce&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Aggregate all intermediate results for a given key&lt;/li&gt;
&lt;li&gt;Produce final output records&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;So Word Count in &lt;strong&gt;&lt;code&gt;Hadoop MapReduce&lt;/code&gt;&lt;/strong&gt; (similar to Kafka broker):
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/mapreduce_wordcount.png&#34;
           alt=&#34;mapreduce word count&#34; width=&#34;70%&#34;/&gt;
  &lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;In &lt;em&gt;input split&lt;/em&gt;, the key value is the byte offset.&lt;/li&gt;
&lt;li&gt;In &lt;em&gt;mapping&lt;/em&gt;, we control the hash function such that we make sure same words get the same hash values and words of the same hash value will be sent to the same machine.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;job-components&#34;&gt;Job Components&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Definition&lt;/strong&gt;: A MapReduce &lt;strong&gt;Job&lt;/strong&gt; is a unit of work that the user wants to be performed. The &lt;em&gt;job&lt;/em&gt; consists of:
&lt;ul&gt;
&lt;li&gt;MapReduce program&lt;/li&gt;
&lt;li&gt;Input data set&lt;/li&gt;
&lt;li&gt;Configuration information
Example command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hadoop jar /usr/lib/hadoop/hadoop-streaming.jar &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -file mapper.py &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -mapper &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;python mapper.py&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -file reducer.py &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -reducer &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;python reducer.py&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -input /data/*/ &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -output /OutputFolder
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;Hadoop&lt;/code&gt; &lt;strong&gt;keeps the source code in the local cluster&lt;/strong&gt;, but data in HDFS. Because when we are dealing with Big data, we keep the data where they are and we move the source codes, processes to the place where the data are stored. This is called &lt;em&gt;&lt;strong&gt;Data localization&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-flow---map&#34;&gt;Data Flow - Map&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The Input data are divided into fixed-size pieces called splits
&lt;ul&gt;
&lt;li&gt;One map task is created for each split&lt;/li&gt;
&lt;li&gt;The map function is run on each split&lt;/li&gt;
&lt;li&gt;Configuration information indicates where the input and the output are stored.&lt;/li&gt;
&lt;li&gt;The maximum number of Mappers you can have in an application is equal to the number of your input data splits, because each input split is processed by exactly one Mapper.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;from-map-to-reduce&#34;&gt;From Map to Reduce&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Select a number R of reduce tasks.&lt;/li&gt;
&lt;li&gt;Divide the intermediate keys into R groups,&lt;/li&gt;
&lt;li&gt;Map tasks use efficient hashing function to group keys that should be sent to the same reducer.&lt;/li&gt;
&lt;li&gt;Each map task transfers each group of keys to one of the R reducer’s container&lt;/li&gt;
&lt;li&gt;Once all mapper task results are transferred to the reducer, the reducer merges the results and computes the final output&lt;/li&gt;
&lt;li&gt;All values with the same key are reduced together. The reducer then writes the output to HDFS&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;decomposing-a-problem&#34;&gt;Decomposing a Problem&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Map&lt;/strong&gt; task focuses on:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Extracting&lt;/strong&gt; the key and temperature from each line in each file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Emitting&lt;/strong&gt; intermediate key-value pairs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Hadoop will accumulate all the values for a given key and pass them to a specific &lt;strong&gt;Reducer&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reducer&lt;/strong&gt; task focuses on:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Aggregating&lt;/strong&gt; all the values for a given key
&lt;ul&gt;
&lt;li&gt;Find the maximum temperature for a given year in the NCDC Example&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Emitting&lt;/strong&gt; final key-value pairs. Writing the output to HDFS&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;read-output&#34;&gt;Read Output&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Each reducer writes the output in a separate file on HDFS.&lt;/li&gt;
&lt;li&gt;Merge the output files and copy them to your local cluster:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hadoop fs -getmerge /OutputFolder/ local_output
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cat local_output
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;improvements&#34;&gt;Improvements&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Combiner&lt;/strong&gt; function:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A mini-reducer that performs in-memory local aggregation of the intermediate outputs after the map phase (inside the memory of the &lt;em&gt;Map&lt;/em&gt; tasks) and before the shuffle phase.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reduces the amount of intermediate data&lt;/strong&gt; (key/value pair -&amp;gt; network traffic) transferred from the Mapper to the Reducer&lt;/li&gt;
&lt;li&gt;But if there are &lt;strong&gt;too many hetero elements&lt;/strong&gt;, it’s no longer efficient which means it is almost like doing another reduce task on the map node.&lt;/li&gt;
&lt;li&gt;Example code:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hadoop jar /usr/lib/hadoop/hadoop-streaming.jar &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -files temperature_mapper.py,temperature_reducer.py &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -input /data/*/ &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -output /NewTempOutFolder &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -mapper &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;python temperature_mapper.py&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -combiner &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;python temperature_reducer.py&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -reducer &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;python temperature_reducer.py&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Partitioner&lt;/strong&gt;(k&amp;rsquo;(key), num_of_partitions=num_of_reducer) -&amp;gt; partition for k&#39;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Controls the partitioning of the intermediate map output keys. Splits the map output records &lt;strong&gt;with the same key&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Divides up key space&lt;/strong&gt; for parallel reduce operations&lt;/li&gt;
&lt;li&gt;Executes on each machine that performs a map task as a local partitioning.&lt;/li&gt;
&lt;li&gt;Default partitioner: hash function that computes the hash value of the key and assigns it to a reducer based on the number of reducers.
&lt;ul&gt;
&lt;li&gt;e.g., &lt;code&gt;partition = hash(key) mod num_of_reducers&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Custom partitioner: user-defined function that controls how the keys are distributed to the reducers.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;customization-parameters&#34;&gt;Customization Parameters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Control number of reducers in your app: &lt;code&gt;-numReduceTasks N&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Partitioners can help you create compound keys: &lt;code&gt;-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Comparator specifies a custom method to sorting keys at the shuffle and sort stage. Example:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;yarn jar /opt/hadoop/hadoop-streaming.jar &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -D mapreduce.job.output.key.comparator.class&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\ &lt;/span&gt;&amp;lt;!-- Specify a custom key comparator class --&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  org.apache.hadoop.mapred.lib.partition.KeyFieldBasedComparator &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  &amp;lt;!-- treat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;.&amp;#34;&lt;/span&gt; as the delimiter between key fields. --&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  -D mapreduce.map.output.key.field.separator&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;. &lt;span style=&#34;color:#ae81ff&#34;&gt;\ &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &amp;lt;!-- -k2,2: sort by field &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; only; n: numeric sort; r: reverse order&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;descending&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; --&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &amp;lt;!-- Sort keys by their second field numerically in descending order. --&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  -D mapreduce.partition.keycomparator.options&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;-k2,2nr &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;Reading: more on Partitioners and Comparators: &lt;a href=&#34;https://www.softcover.io/read/f0b3ea72/introbigdata_book/hadoop_optimized&#34;&gt;https://www.softcover.io/read/f0b3ea72/introbigdata_book/hadoop_optimized&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
- http://allenhsm.github.io/tech/cloud_infra_basics/hadoop_mapreduce/ - This is a customized copyright.</description>
        </item>
    
    
    
        <item>
        <title>Hadoop - HDFS</title>
        <link>http://allenhsm.github.io/tech/cloud_infra_basics/hadoop_hdfs/</link>
        <pubDate>Sat, 29 Nov 2025 01:27:18 -0500</pubDate>
        
        <guid>http://allenhsm.github.io/tech/cloud_infra_basics/hadoop_hdfs/</guid>
        <description>Allen&#39;s Blog http://allenhsm.github.io/tech/cloud_infra_basics/hadoop_hdfs/ -&lt;h1 id=&#34;intro&#34;&gt;&lt;strong&gt;Intro&lt;/strong&gt;&lt;/h1&gt;
&lt;h2 id=&#34;motivation-for-hadoop&#34;&gt;Motivation for Hadoop&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Handle &lt;em&gt;&lt;strong&gt;big data!&lt;/strong&gt;&lt;/em&gt; that exceeds single machine&amp;rsquo;s storage and computing capacity&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Storage and Analysis&lt;/strong&gt;&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt; capacity increases faster than then access speeds.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Need &lt;em&gt;&lt;strong&gt;parallel data access&lt;/strong&gt;&lt;/em&gt; to get things done quickly
&lt;ul&gt;
&lt;li&gt;1 machine accessing 1000 GB is much slower than 100 machines, each is accessing 10 GB.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Shared access&lt;/strong&gt;&lt;/em&gt; for efficiency and scalability&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;challenges&#34;&gt;Challenges&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Analysis tasks need to combine data from multiple sources
&lt;ul&gt;
&lt;li&gt;Need a paradigm that &lt;strong&gt;transparently splits and merges data&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Challenge of parallel data access to and from multiple disks
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hardware failure&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;why-hadoop&#34;&gt;Why Hadoop&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We need Open-source software framework for storing data and running applications on clusters of commodity hardware -&amp;gt; Hadoop is &lt;em&gt;&lt;strong&gt;cheap&lt;/strong&gt;&lt;/em&gt; to implement and expand&lt;/li&gt;
&lt;li&gt;Provides massive storage for any kind of data, enormous processing power and the ability to handle virtually limitless concurrent tasks or jobs -&amp;gt; Hadoop is &lt;em&gt;&lt;strong&gt;scalable&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;hadoop-basics&#34;&gt;Hadoop Basics&lt;/h1&gt;
&lt;h2 id=&#34;component-architecture&#34;&gt;Component Architecture&lt;/h2&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/hadoop_architecture.png&#34;
         alt=&#34;hadoop architecture&#34; width=&#34;60%&#34;/&gt;
&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;HDFS: Hadoop Distributed File System
&lt;ul&gt;
&lt;li&gt;Designed to provide &lt;em&gt;&lt;strong&gt;highly fault-tolerant&lt;/strong&gt;&lt;/em&gt; and to be deployed on &lt;em&gt;&lt;strong&gt;low-cost&lt;/strong&gt;&lt;/em&gt; hardware&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MapReduce: A framework for processing data in &lt;em&gt;&lt;strong&gt;batch&lt;/strong&gt;&lt;/em&gt; - &lt;code&gt;BSP&lt;/code&gt; (Bulk Synchronous Parallel)
&lt;ul&gt;
&lt;li&gt;Enables &lt;em&gt;&lt;strong&gt;distributed processing&lt;/strong&gt;&lt;/em&gt; of large data sets across clusters of computers using simple programming models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;history&#34;&gt;History&lt;/h2&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/hadoop_history.png&#34;
         alt=&#34;hadoop history&#34; width=&#34;100%&#34;/&gt;
&lt;/figure&gt;

&lt;h1 id=&#34;hdfs&#34;&gt;&lt;strong&gt;HDFS&lt;/strong&gt;&lt;/h1&gt;
&lt;h2 id=&#34;distributed-file-system&#34;&gt;&lt;strong&gt;Distributed File System&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;em&gt;&lt;strong&gt;client/server-based&lt;/strong&gt;&lt;/em&gt; application that allows clients to access and process data stored on the server as if they were on their own computer
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Client/Server&lt;/strong&gt; arch: Clients request services and resources from a centralized server. Client drives the interaction.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Master/Worker&lt;/strong&gt; arch: Master node controls and manages the worker nodes. Master coordinates and distributes jobs; workers execute computation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;More complex than regular disk file systems: &lt;em&gt;&lt;strong&gt;network based&lt;/strong&gt;&lt;/em&gt; and high level of &lt;em&gt;&lt;strong&gt;fault tolerance&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Main motivation: &lt;em&gt;&lt;strong&gt;BIG DATA&lt;/strong&gt;&lt;/em&gt;: datasets outgrows the storage capacity of a single physical machine, so data are partitioned across separate machines&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;basics&#34;&gt;Basics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;HDFS is a file system designed for storing &lt;em&gt;&lt;strong&gt;very large files&lt;/strong&gt;&lt;/em&gt; with &lt;em&gt;&lt;strong&gt;streaming data access&lt;/strong&gt;&lt;/em&gt; patterns, running on clusters that are using &lt;em&gt;&lt;strong&gt;commodity hardware&lt;/strong&gt;&lt;/em&gt;.
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Very large files&lt;/strong&gt;: files are normally hundreds of megabytes, gigabytes, or terabytes in size.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Streaming data access&lt;/strong&gt;: HDFS is designed more for batch processing, a &lt;em&gt;write-once, read many-times&lt;/em&gt; pattern&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Commodity hardware&lt;/strong&gt;: node failure probability is &lt;strong&gt;high&lt;/strong&gt;, so we need a mechanism that tolerates failures without disruption or loss of data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;structure&#34;&gt;&lt;strong&gt;Structure&lt;/strong&gt;&lt;/h2&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/hdfs_design.png&#34;
         alt=&#34;hdfs design&#34; width=&#34;80%&#34;/&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;namenodemaster&#34;&gt;&lt;em&gt;&lt;strong&gt;Namenode&lt;/strong&gt;&lt;/em&gt;(master)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Maintains the &lt;strong&gt;file system tree&lt;/strong&gt; and the &lt;strong&gt;metadata&lt;/strong&gt; for all the files and directories in the tree. Store persistently on the local disk in the form of two files: the &lt;em&gt;&lt;strong&gt;namespace image&lt;/strong&gt;&lt;/em&gt; and the &lt;em&gt;&lt;strong&gt;edit log&lt;/strong&gt;&lt;/em&gt;()
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Namespace image&lt;/strong&gt; aka &lt;strong&gt;fsimage&lt;/strong&gt;: the entire HDFS directory tree and block locations, representing all metadata at a point in time&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Edit log&lt;/strong&gt;: a transaction log that records every change that occurs to the file system metadata &lt;em&gt;&lt;strong&gt;after&lt;/strong&gt;&lt;/em&gt; the fsimage snapshot&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;When a fail happens, we create a new Namenode and try to make it into the same state of previous nodes before failure using the two data above.&lt;/li&gt;
&lt;li&gt;Services &lt;strong&gt;block location&lt;/strong&gt; requests from clients&lt;/li&gt;
&lt;li&gt;The entire &lt;code&gt;(block, datanode)&lt;/code&gt; mapping table is stored in memory, for efficient access. &lt;code&gt;Namespace image&lt;/code&gt;(aka &lt;code&gt;fsimage&lt;/code&gt;) loaded into RAM on startup of Namenode.&lt;/li&gt;
&lt;li&gt;To deal with the great amount of log data writing into the &lt;code&gt;edit log&lt;/code&gt;, the Namenode &lt;em&gt;&lt;strong&gt;periodically merges&lt;/strong&gt;&lt;/em&gt; the &lt;code&gt;edit log&lt;/code&gt; with the &lt;code&gt;namespace image&lt;/code&gt; to create a new &lt;code&gt;namespace image&lt;/code&gt;, and clears the &lt;code&gt;edit log&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Done by &lt;code&gt;secondary namenode&lt;/code&gt; or &lt;code&gt;checkpoint node&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Namenode 内部的i/O操作如写日志、快照是异步的，多个客户端写操作可批量处理，bing发请求通过 &lt;strong&gt;RPC 队列 + 多线程处理&lt;/strong&gt;，提升吞吐。&lt;/li&gt;
&lt;li&gt;High availability mechanism:  生产集群一般启用 &lt;strong&gt;Active / Standby NameNode&lt;/strong&gt;；共享 EditLog（通常存储在 JournalNodes 上）；当 Active NameNode 压力过大或宕机时，Standby 可自动接管。&lt;/li&gt;
&lt;li&gt;For scalability issue, we introduce &lt;strong&gt;Namenode Federation&lt;/strong&gt; by adding more namenodes. Each namenode manages a portion of the file system namespace.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;datanode&#34;&gt;&lt;em&gt;&lt;strong&gt;Datanode&lt;/strong&gt;&lt;/em&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Stores HDFS file &lt;code&gt;blocks&lt;/code&gt; as &lt;em&gt;&lt;strong&gt;independent files&lt;/strong&gt;&lt;/em&gt; in the local file system&lt;/li&gt;
&lt;li&gt;Handles client read and write operations&lt;/li&gt;
&lt;li&gt;Maintains the block location info by sending &lt;em&gt;&lt;strong&gt;heartbeats&lt;/strong&gt;&lt;/em&gt; and &lt;em&gt;&lt;strong&gt;block reports&lt;/strong&gt;&lt;/em&gt; to the Namenode periodically&lt;/li&gt;
&lt;li&gt;Re-replicates blocks when instructed by the NameNode&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-blocks&#34;&gt;&lt;strong&gt;Data Blocks&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Why streaming data?
&lt;ul&gt;
&lt;li&gt;Since we want the cost to write is very cheap, the cost of hardware is cheap. Too cheap to do multiple write on the same location&lt;/li&gt;
&lt;li&gt;Streaming data access allows us to optimize for &lt;em&gt;&lt;strong&gt;high throughput&lt;/strong&gt;&lt;/em&gt; of data access, rather than low latency of data access&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Why large files?
&lt;ul&gt;
&lt;li&gt;Unlike mechanical disks, the &lt;code&gt;block&lt;/code&gt; size in HDFS is &lt;em&gt;&lt;strong&gt;128MB&lt;/strong&gt;&lt;/em&gt; by default, such that the head of the disk do not need to switch so often between different block to read the same file. In this way it &lt;em&gt;&lt;strong&gt;minimizes the cost of seek time&lt;/strong&gt;&lt;/em&gt;(the time to move the head assembly to the correct cylinder) &amp;lt;=1% of the disk transfer time&lt;/li&gt;
&lt;li&gt;Files in HDFS are broken into &lt;strong&gt;block-sized chunks&lt;/strong&gt;, which are stored as independent units. If the size of the file is less than the HDFS block size, the file does not occupy the complete block storage&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;architectural-layers&#34;&gt;&lt;strong&gt;Architectural Layers&lt;/strong&gt;&lt;/h2&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/hdfs_layers.png&#34;
         alt=&#34;hdfs layers&#34; width=&#34;30%&#34;/&gt;
&lt;/figure&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt; Layer (&lt;code&gt;DataNodes&lt;/code&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Stores the actual file blocks.&lt;/li&gt;
&lt;li&gt;Equivalent to the “hard drive” layer.&lt;/li&gt;
&lt;li&gt;Distributed across many machines.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Metadata&lt;/strong&gt; Layer (&lt;code&gt;NameNode&lt;/code&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Stores namespace metadata:
directories, filenames, permissions, block maps.&lt;/li&gt;
&lt;li&gt;Equivalent to the “file system brain”.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;Coordination&lt;/strong&gt; Layer (&lt;code&gt;Secondary/Checkpoint&lt;/code&gt; Nodes)&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Merges fsimage + edit logs&lt;/li&gt;
&lt;li&gt;Keeps NameNode healthy and restartable.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;Client&lt;/strong&gt; Layer&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Users and applications interacting with HDFS.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;namespace&#34;&gt;&lt;strong&gt;Namespace&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Consists of directories, files and blocks.&lt;/li&gt;
&lt;li&gt;Supports all the namespace related file system operations such as create, delete, modify and list files and directories.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;block-storage-service&#34;&gt;&lt;strong&gt;Block Storage Service&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Block Management&lt;/strong&gt;&lt;/em&gt; - performed in the &lt;code&gt;Namenode&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Provides &lt;code&gt;Datanode&lt;/code&gt; cluster membership by handling registrations, and &lt;strong&gt;periodic heart beats&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Processes &lt;strong&gt;block reports&lt;/strong&gt; and maintains location of blocks.&lt;/li&gt;
&lt;li&gt;Supports block related operations such as create, delete, modify and get block location.&lt;/li&gt;
&lt;li&gt;Manages replica placement and block replication.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/em&gt; - provided by &lt;code&gt;Datanode&lt;/code&gt;, mentioned above: HDFS - structure - Datanode&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;namenode-issues&#34;&gt;&lt;strong&gt;Namenode Issues&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Namenode maintains a single namespace and metadata of all the blocks for the entire cluster
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Not scalable&lt;/strong&gt;&lt;/em&gt;: the entire namespace and block metadata are stored in memory&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Poor isolation&lt;/strong&gt;&lt;/em&gt;: there is no way to separate a group of works from one another&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Namenode is the main component to make filesystem &lt;strong&gt;operational&lt;/strong&gt;. If it fails, the whole cluster will not function&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solutions&#34;&gt;&lt;strong&gt;Solutions&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Namenode Federation - For scalability&lt;/strong&gt; - Hadoop 3.x:
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/hdfs_namenode_federation.png&#34;
         alt=&#34;hdfs federation&#34; width=&#34;40%&#34;/&gt;
&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;introduced in the 2.x Hadoop release to address scalability issues by adding more Namenode(s)&lt;/li&gt;
&lt;li&gt;Each Namenode manages &lt;strong&gt;a portion of filesystem &lt;code&gt;namespace&lt;/code&gt;&lt;/strong&gt; which is independent from other portions handled by other Namenodes.&lt;/li&gt;
&lt;li&gt;Separate the namespace into many small namespaces and dedicated by the master namespace that holds the &lt;code&gt;viewFS&lt;/code&gt; table (mount table) such that they know a folder is maintained by which namespace. Each namespace belong to a namenode, and each namespace can be communicating with several datanodes.
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;HDFS Federation&lt;/code&gt; splits the &lt;code&gt;directory tree&lt;/code&gt;:
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;/user       → namespace A
/data       → namespace B
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;There is a top-level ‘virtual’ namespace that acts like the master router. It doesn’t store data—it only stores a &lt;em&gt;&lt;strong&gt;mount table&lt;/strong&gt;&lt;/em&gt;:
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;/user     → maps to namespace A (NameNode A)
/data     → maps to namespace B (NameNode B)
&lt;/code&gt;&lt;/pre&gt;This master namespace uses ViewFS to route requests to the correct NameNode. So When you access &lt;code&gt;/data/file1&lt;/code&gt;, the ViewFS mount table recognizes: → &lt;code&gt;/data&lt;/code&gt; belongs to Namespace B → send request to NameNode B.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Each Datanode will register with each Namenode to store blocks for different namespace. Each NameNode is allowed to talk to &lt;strong&gt;any DataNode&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Full structure:
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;    +-----------------------+
    |     ViewFS (mount)    |
    |  &amp;#34;/user&amp;#34; → NN1        |
    |  &amp;#34;/data&amp;#34; → NN2        |
    +-----------------------+
    /           |            \
    NN1         NN2           NN3    (many namespaces)
    / \         / \           / \
DN1 DN2     DN1 DN3       DN2 DN3  (shared DataNode pool)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Namenode Fault Tolerance - For failures&lt;/strong&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/hdfs_secondary_namenode.png&#34;
           alt=&#34;hdfs ha namenode&#34; width=&#34;52%&#34;/&gt;
  &lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;Periodically (hourly) &lt;strong&gt;pulls a copy of the file metadata&lt;/strong&gt; from the active &lt;code&gt;Namenode&lt;/code&gt; to save to its local disk&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Problem&lt;/strong&gt;: Secondary Namenode might take too long to come up online (more than &lt;strong&gt;30 minutes&lt;/strong&gt;):
&lt;ul&gt;
&lt;li&gt;Load the namespace image into the memory&lt;/li&gt;
&lt;li&gt;Replay its edit log&lt;/li&gt;
&lt;li&gt;Receive enough block reports from the Datanodes to leave safe-mode&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Namenode Fault Tolerance - High Availability (HA) Configuration&lt;/strong&gt; - Hadoop 3.x
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/hdfs_ha_namenode.png&#34;
           alt=&#34;hdfs ha namenode 2&#34; width=&#34;60%&#34;/&gt;
  &lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;A pair of NN is configured as &lt;strong&gt;active-standby&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The standby takes over whenever the active one fails&lt;/li&gt;
&lt;li&gt;The Namenodes use highly available shared storage to share the edit log&lt;/li&gt;
&lt;li&gt;The active Namenode writes, and the standby Namenode reads to keep it in sync&lt;/li&gt;
&lt;li&gt;Datanodes must send block reports to both Namenodes&lt;/li&gt;
&lt;li&gt;Client must be configured to handleNamenode failover&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;datanode-failure&#34;&gt;&lt;strong&gt;Datanode failure&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Datanode Fault Tolerance - &lt;em&gt;Replication Factors&lt;/em&gt;&lt;/strong&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/hdfs_datanode_replica.png&#34;
           alt=&#34;hdfs datanode failure&#34; width=&#34;45%&#34;/&gt;
  &lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;Each data blocks are replicated (&lt;strong&gt;thrice&lt;/strong&gt; by default) and are distributed across different DataNodes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;key-takeaways&#34;&gt;Key Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;HDFS is for &lt;em&gt;&lt;strong&gt;high thoughput&lt;/strong&gt;&lt;/em&gt;, not low latency.&lt;/li&gt;
&lt;li&gt;Not for a large number of small files but for &lt;em&gt;&lt;strong&gt;large files&lt;/strong&gt;&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;Large number of small files: Costly metadata&lt;/li&gt;
&lt;li&gt;In average, each file, directory, and block takes about 150 bytes. For a HDFS that maintains 1 million files with one block each will need memory – 1,000,000&lt;em&gt;150&lt;/em&gt;2 = 300 MB&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Is for applications with &lt;em&gt;&lt;strong&gt;single writer&lt;/strong&gt;&lt;/em&gt; and always make &lt;em&gt;&lt;strong&gt;writes at the end of file&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
- http://allenhsm.github.io/tech/cloud_infra_basics/hadoop_hdfs/ - This is a customized copyright.</description>
        </item>
    
    
    
        <item>
        <title>Concepts in Cloud Infra</title>
        <link>http://allenhsm.github.io/tech/cloud_infra_basics/concepts_in_cloud_infra/</link>
        <pubDate>Fri, 28 Nov 2025 19:28:43 -0500</pubDate>
        
        <guid>http://allenhsm.github.io/tech/cloud_infra_basics/concepts_in_cloud_infra/</guid>
        <description>Allen&#39;s Blog http://allenhsm.github.io/tech/cloud_infra_basics/concepts_in_cloud_infra/ -&lt;h1 id=&#34;cloud-types&#34;&gt;Cloud Types&lt;/h1&gt;
&lt;h2 id=&#34;public-cloud&#34;&gt;Public Cloud&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Services offered over the public internet and available to anyone who wants to purchase them. Typically owned and operated by third-party cloud service providers (e.g., AWS, Azure, Google Cloud). It exists on the premise of the cloud service provider&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Advantage&lt;/strong&gt;: cost. &lt;strong&gt;Concern&lt;/strong&gt;: Security. However there are some public cloud providers that have demonstrated strong security measures.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;private-cloud&#34;&gt;Private Cloud&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Implemented within the internal IT environment of the company, typically managed by the company or a third party vendor&lt;/li&gt;
&lt;li&gt;In private clouds, the servers and storage devices may exist on premist or off-premise&lt;/li&gt;
&lt;li&gt;Private clouds can deliver IaaS internally to employees or business units through an intranet or the Internet via a virtual private network (VPN), as well as software (applications) or storage as services to its branch offices&lt;/li&gt;
&lt;li&gt;Examples of services delivered through the private cloud include database on demand, email on demand, and storage on demand.&lt;/li&gt;
&lt;li&gt;A key motivation for opting-in for a private cloud is &lt;em&gt;&lt;strong&gt;security&lt;/strong&gt;&lt;/em&gt;. A private cloud infrastructure offers tighter controls over the geographic location of data storage and other aspects of security. Other benefits include easy resource sharing and rapid deployment to organizational entities.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;hybrid-cloud&#34;&gt;Hybrid cloud&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In Hybrid Cloud, constituting private and public clouds are bound together by standardized technology that enables data and application portability (e.g., cloud bursting for load balancing between clouds).&lt;/li&gt;
&lt;li&gt;With a hybrid cloud solution, sensitive information can be placed in a private area of the cloud, and less sensitive data can take advantage of the benefits of the public cloud. So Using a hybrid cloud requires proper management of data transfers and replication between public and private clouds&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;comparison&#34;&gt;Comparison&lt;/h2&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/cloud_comparison.png&#34;
         alt=&#34;cloud types&#34; width=&#34;60%&#34;/&gt;
&lt;/figure&gt;

&lt;h1 id=&#34;data-centers&#34;&gt;Data Centers&lt;/h1&gt;
&lt;h2 id=&#34;elements&#34;&gt;Elements:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Facility&lt;/strong&gt;: location and &amp;ldquo;white space&amp;rdquo; (space for IT equipment)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IT Equipment&lt;/strong&gt;: servers, storage hardware, cables, racks, firewalls, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Support Infra&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Uninterruptible Power Sources (UPS)&lt;/strong&gt;&lt;/em&gt;: battery banks, redundant power sources and generators&lt;/li&gt;
&lt;li&gt;Environmental controls: cooling systems, fire suppression systems&lt;/li&gt;
&lt;li&gt;Physical security systems: surveillance cameras, biometric access controls&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Operations Staff&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;clouds-on-data-centers&#34;&gt;Clouds on Data Centers&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Range in size from &lt;em&gt;edge&lt;/em&gt; facilities to &lt;em&gt;mega&lt;/em&gt; scale&lt;/li&gt;
&lt;li&gt;Economies of scale: approximate costs for a small size center (1000 servers) and a larger, 100K server center
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/data_center_costs.png&#34;
         alt=&#34;data center costs&#34; width=&#34;90%&#34;/&gt;
&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-center-efficiency&#34;&gt;Data Center Efficiency&lt;/h2&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/data_center_efficiency.png&#34;
         alt=&#34;data center efficiency&#34; width=&#34;60%&#34;/&gt;
&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;The Green Grid, an association of IT professionals focused on increasing the energy efficiency of data centers, created two major metrics to assess the efficiency of data center efficiency.&lt;/li&gt;
&lt;li&gt;These metrics will help organizations make better decisions while deploying new data centers or replacing existing parts of current data centers.
&lt;ul&gt;
&lt;li&gt;&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/data_center_efficiency_metrics.png&#34;
         alt=&#34;data center efficiency metrics&#34; width=&#34;70%&#34;/&gt;
&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Data Center Infrastructure Efficiency (&lt;em&gt;DCIE&lt;/em&gt;) = &lt;code&gt;1 / PUE&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;To improve DC Deployment: since Building racks of servers &amp;amp; complex cooling systems all separately is not efficient, they package and deploy into &lt;em&gt;bigger units&lt;/em&gt;, like &lt;code&gt;ICE Cube&lt;/code&gt; as a modular data center.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;cloud-computing-limitations&#34;&gt;Cloud Computing Limitations&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Connectivity&lt;/strong&gt;&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;Connectivity is needed.&lt;/li&gt;
&lt;li&gt;Some devices need to be continuously operable even when not connected.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Bandwidth&lt;/strong&gt;&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;How much data do we create every day? In 2019, The World Economic Forum reports that the entire digital world is expected to reach 44 ettabytes by 2020.&lt;/li&gt;
&lt;li&gt;Eventually, when all devices are connected to the cloud, bandwidth may become a big issue&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Centralization of Analytics
&lt;ul&gt;
&lt;li&gt;Data should be stored on the cloud before reports can be available&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Security&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;solutions&#34;&gt;Solutions&lt;/h2&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/cloud_computing_limitations_solutions.png&#34;
         alt=&#34;solution to cloud computing limitations&#34; width=&#34;80%&#34;/&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;fog-computing&#34;&gt;&lt;em&gt;Fog Computing&lt;/em&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Fog computing&lt;/strong&gt;&lt;/em&gt; is an extra layer between the edge layer and the cloud layer.&lt;/li&gt;
&lt;li&gt;The main benefit of Fog computing is &lt;strong&gt;efficiency of data traffic&lt;/strong&gt; and a &lt;strong&gt;reduction in latency&lt;/strong&gt;.
&lt;ul&gt;
&lt;li&gt;Data generated from the edge are sent to a fog node close to the data source. These data are analyzed locally, filtered, and then sent to the cloud for long-term storage if necessary.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;According to Mung Chiang, Dean of the Purdue University, “fog provides the missing link for what data needs to be pushed to the cloud, and what can be analyzed locally, at the edge.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;edge-computing&#34;&gt;&lt;em&gt;Edge Computing&lt;/em&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Edge computing is the data computation that happens at the network’s edge where the data are created.&lt;/li&gt;
&lt;li&gt;Fog computing acts as a &lt;strong&gt;mediator&lt;/strong&gt; between the edge and the cloud for several purposes, such as data filtering.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Fog computing can’t replace edge computing&lt;/strong&gt;&lt;/em&gt; while edge computing can live without fog computing in many applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comparison-1&#34;&gt;Comparison&lt;/h3&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/cloud_fog_comparison.png&#34;
         alt=&#34;fog vs edge computing&#34; width=&#34;80%&#34;/&gt;
&lt;/figure&gt;

- http://allenhsm.github.io/tech/cloud_infra_basics/concepts_in_cloud_infra/ - This is a customized copyright.</description>
        </item>
    
    
    
        <item>
        <title>IaC &amp; Terraform</title>
        <link>http://allenhsm.github.io/tech/cloud_infra_basics/terraform/</link>
        <pubDate>Fri, 28 Nov 2025 00:04:22 -0500</pubDate>
        
        <guid>http://allenhsm.github.io/tech/cloud_infra_basics/terraform/</guid>
        <description>Allen&#39;s Blog http://allenhsm.github.io/tech/cloud_infra_basics/terraform/ -&lt;h1 id=&#34;infrastructure-as-code&#34;&gt;Infrastructure as Code&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Definition&lt;/strong&gt;: Write and execute code to define, deploy update, and destroy our infra.&lt;/li&gt;
&lt;li&gt;Why we automate deployment? reliable, efficient, repeatable, safe releases, multiple environment, maximum scalability&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;types-of-iac-tools&#34;&gt;Types of IaC tools&lt;/h2&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/types_of_iac.png&#34;
         alt=&#34;IaC Tools&#34; width=&#34;50%&#34;/&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;ad-hoc-scripts&#34;&gt;Ad-hoc Scripts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;easiest&lt;/strong&gt; and most straightforward approach of automating anything&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ad-hoc tools&lt;/strong&gt; run on single core tool, like cloud shell, which connects to a VM with only 5GB storage space, if you need more, you have to upload the file to cloud bucket or sth, and use the shell to read from it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;examples&lt;/strong&gt; : Bash, Ruby, Perl&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;configuration-management-tools&#34;&gt;Configuration Management Tools&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Designed to install and manage software on existing servers.&lt;/li&gt;
&lt;li&gt;These tools offer coding conventions and ability to run scripts on multiple servers.&lt;/li&gt;
&lt;li&gt;ansible assumes the multiple servers that you use work in completely the same way with same hard tool structure. In these cases, even though the servers are of the same, some external small errors, like a sudden network error, cause the machines to diverge in some way, like having different file system structure.&lt;/li&gt;
&lt;li&gt;Examples: Chef, Puppet and Ansible.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;server-templating-tools&#34;&gt;Server Templating Tools&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The idea of server templating tools is to create an image of a server that captures a fully self-contained “snapshot” of the operating system (OS), the software, the files, and all other relevant details.&lt;/li&gt;
&lt;li&gt;Examples: Docker, Packer, Vagrant&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;provisioning-tools&#34;&gt;Provisioning Tools&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Most &lt;strong&gt;advanced&lt;/strong&gt; IaC tools.&lt;/li&gt;
&lt;li&gt;Provisioning tools are responsible for &lt;strong&gt;creating&lt;/strong&gt; these servers, rather than running software or scripts on top of existing servers like other categories.&lt;/li&gt;
&lt;li&gt;You can use provisioning tools to create databases, caches, load balancers, queues, network configurations and SSL certificates.&lt;/li&gt;
&lt;li&gt;Examples: Terraform, CloudFormation, OpenStack Heat, and Pulumi, actively deal with hardware&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;orchestration-tools&#34;&gt;Orchestration Tools&lt;/h3&gt;
&lt;p&gt;We mentioned previously. Orchestration tools are responsible for managing the lifecycle of containers, including deployment, scaling, and networking.&lt;/p&gt;
&lt;h2 id=&#34;iac-tool-comparison&#34;&gt;IaC Tool Comparison&lt;/h2&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/iac_comparison.png&#34;
         alt=&#34;IaC Tool Comparison&#34; width=&#34;100%&#34;/&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;mutable-vs-_immutable_&#34;&gt;Mutable vs. &lt;em&gt;&lt;strong&gt;Immutable&lt;/strong&gt;&lt;/em&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mutable&lt;/strong&gt;: Configuration management tools (e.g., chef, puppet) default to a mutable infrastructure paradigm.
&lt;ul&gt;
&lt;li&gt;like ansible &amp;amp; chef where there is possible mutation to the softwares like the network error mentioned above that causes the infra to diverge between machines.&lt;/li&gt;
&lt;li&gt;Over time, each server becomes slightly different than all the others, leading to subtle configuration bugs that are &lt;strong&gt;difficult to diagnose and reproduce.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Immutable&lt;/strong&gt; infrastructure tools don’t allow this deviation to happen.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;declarative-vs-procedural&#34;&gt;&lt;em&gt;&lt;strong&gt;Declarative&lt;/strong&gt;&lt;/em&gt; vs. Procedural&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In &lt;strong&gt;Procedural&lt;/strong&gt; scripting style, developers will code the application flow in step-by-step commands that does not care about the outcome or the final state, but just complete the preset steps. e.g. chef and ansible&lt;/li&gt;
&lt;li&gt;In &lt;strong&gt;Declarative&lt;/strong&gt; scripting style, developers will write code that specifies their desired end state, and the IaC tool itself is responsible for figuring out how to achieve that state.
&lt;ul&gt;
&lt;li&gt;e.g. 5 servers, 3 firewall connections, 1 SSL certificate, IaC tool is responsible for figuring out how to achieve these goals. E.g. Terraform, CloudFormation (provisioning), puppet(config management)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;general-purpose-vs-domain-specific&#34;&gt;&lt;em&gt;&lt;strong&gt;General Purpose&lt;/strong&gt;&lt;/em&gt; vs. Domain-specific&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;DSLs&lt;/code&gt; are designed for use in &lt;strong&gt;one specific domain&lt;/strong&gt;, whereas &lt;code&gt;GPLs&lt;/code&gt; can be used across a &lt;strong&gt;broad range of domains&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GPL&lt;/code&gt; Examples: Chef supports Ruby; Pulumi supports a wide variety of GPLs, including JavaScript, TypeScript, Python, Go, C#, Java, and others.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DSL&lt;/code&gt; Examples: Terraform uses HCL; Puppet uses Puppet Language; Ansible, CloudFormation, and OpenStack Heat use YAML (CloudFormation also supports JSON).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;master-vs-masterless&#34;&gt;Master vs. &lt;em&gt;&lt;strong&gt;Masterless&lt;/strong&gt;&lt;/em&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In &lt;strong&gt;Master-centered&lt;/strong&gt; infrastructure tools, you should &lt;strong&gt;dedicate a server&lt;/strong&gt; to run the IaC tool (e.g., Chef, Puppet).
&lt;ul&gt;
&lt;li&gt;Every time you want to update something in your infra, you use a client (e.g., a command-line tool) to issue new commands to the master server, and the master server either pushes the updates out to all the other servers or those servers pull the latest updates down from the master server on a regular basis.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;In &lt;strong&gt;masterless&lt;/strong&gt; infrastructure tools, tools don’t require a Master or rely on a master server that is part of the existing infra.
&lt;ul&gt;
&lt;li&gt;For example, Terraform communicates with cloud providers using the cloud provider’s APIs, so in some sense, the API servers are master servers, except that they don’t require any extra infrastructure or any extra authentication mechanisms (i.e., just use your API keys).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;agent-k8s-vs-agentless&#34;&gt;Agent (k8s) vs. &lt;em&gt;&lt;strong&gt;Agentless&lt;/strong&gt;&lt;/em&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Some IaC tools require the installation of agent software on each node you need to configure. The agent typically runs in the background on each server and is responsible for installing the latest configuration management updates.
&lt;ul&gt;
&lt;li&gt;This introduces a security concern since you need to open the outbound port for the agent to communicate with the master server&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;community-support&#34;&gt;Community Support&lt;/h3&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/iac_community_support.png&#34;
         alt=&#34;IaC Community Support&#34; width=&#34;40%&#34;/&gt;
&lt;/figure&gt;

&lt;h1 id=&#34;terraform&#34;&gt;Terraform&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;An open-source IaC provisioning tool created by HashiCorp and written in GO Lang.&lt;/li&gt;
&lt;li&gt;script automatically detects the environment it is running on (GCP, AWS, etc) and install the corresponding libraries and use a hash map to map the compatible APIs.&lt;/li&gt;
&lt;li&gt;Good readability, can deploy to K8s, declarative, paves the way for massive scalability, Management of infrastructure will be automated, Source of truth for the infra&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;hashicorp-configuration-language-hcl&#34;&gt;HashiCorp Configuration Language (HCL)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The most basic construct is called a &lt;code&gt;block&lt;/code&gt;, defined as a “container for other content”.&lt;/li&gt;
&lt;li&gt;The body of the block is nested with {}&lt;/li&gt;
&lt;li&gt;Each block has a type (e.g., resource)&lt;/li&gt;
&lt;li&gt;Each block is identified using labels, the number of labels is identified based on the block type&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;block-types&#34;&gt;Block types&lt;/h3&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/terraform_block_types.png&#34;
         alt=&#34;Terraform Block Types&#34; width=&#34;90%&#34;/&gt;
&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Resource block&lt;/strong&gt;: most popular; to &lt;strong&gt;create components&lt;/strong&gt; of your infra, like VM instances, databases, and networking components.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Provider block&lt;/strong&gt;: to define the &lt;strong&gt;cloud provider&lt;/strong&gt; (e.g., AWS, GCP, Azure) that you want to use to host and deploy your resources. Terraform decides which mapping table to download based on the cloud provider.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variable block&lt;/strong&gt;: to define &lt;strong&gt;input variables&lt;/strong&gt; that can be used to parameterize your Terraform configuration.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output block&lt;/strong&gt;: exposes information about specific resources created in your configuration.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Provisioner&lt;/strong&gt;: To execute a script &lt;strong&gt;after your resources have been created&lt;/strong&gt;. It waits for your service to be deployed successfully and then run some codes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Local&lt;/strong&gt;: To create local variables within a specific resource block&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;workflow&#34;&gt;Workflow&lt;/h2&gt;
&lt;p&gt;A basic Terraform workflow consists of the following two steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Initialization&lt;/strong&gt;: using &lt;code&gt;terraform init&lt;/code&gt;. Install libraries, binaries executable, for the providers that you have listed in your code. The mapping tables, provider apis, etc. All stored in &lt;code&gt;.terraform&lt;/code&gt; folder.
&lt;ul&gt;
&lt;li&gt;You must initialize Terraform every time you write a new configuration file.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Execution&lt;/strong&gt;.
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Plan&lt;/strong&gt;: a log file stored in CI/CD pipeline, keep track what will happen by comparing what we currently have and what we will do (add, delete, change in the current infrastructure).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Apply&lt;/strong&gt;: Once you have confirmed that the plan does what you intended, then execute the plan and Terraform will provision the cloud resources.&lt;/li&gt;
&lt;li&gt;destroy: to remove all resources created by Terraform. Don’t do it by yourself. 解铃还须系铃人&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;naming-conventions&#34;&gt;Naming conventions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Terraform does &lt;strong&gt;not care about filenames&lt;/strong&gt;. It combines all files with the &lt;code&gt;.tf&lt;/code&gt; extension and then creates an execution plan using all the configurations.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;provider.tf&lt;/code&gt;: contains the provider block declaring provider, which is a translation layer between Terraform and the external API of the provider.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;variables.tf&lt;/code&gt;:
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;create all the input global variables you required in your project.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variables are referenced using the &lt;code&gt;var.&amp;lt;variable_name&amp;gt;&lt;/code&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;When &lt;em&gt;apply&lt;/em&gt; it will scan our code to identify what variables or parameters are missing.&lt;/li&gt;
&lt;li&gt;You can create default values in &lt;em&gt;terraform.tfvars&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;startup.sh&lt;/code&gt;: a bash script that will be executed after the resource is created.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;outputs.tf&lt;/code&gt;: contains all the output blocks that expose information about specific resources created in your configuration.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;main.tf&lt;/code&gt;: contains all the resource blocks that define the actual components of your infrastructure. Actual creation of the resource blocks.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;terraform.tfvars&lt;/code&gt;: There are multiple ways to assign a value to a variable, either through a default value in the declaration or pass it as a value either interactively or via the command-line flag. One common way is to create a file named &lt;code&gt;terraform.tfvars&lt;/code&gt; that contains the variable assignments. You can then use them as &lt;code&gt;var.&amp;lt;variable_name&amp;gt;&lt;/code&gt; in your code.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;configuration-structure&#34;&gt;Configuration Structure&lt;/h2&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/terraform_configuration_structure.png&#34;
         alt=&#34;Terraform File Structure&#34; width=&#34;60%&#34;/&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;state&#34;&gt;State&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A Terraform configuration is a declaration of the desired state, and When you run &lt;code&gt;terraform apply&lt;/code&gt;, Terraform determines the difference between the actual state and the desired state&lt;/li&gt;
&lt;li&gt;Terraform &lt;strong&gt;state&lt;/strong&gt; is the file that tracks the current actual state of your infra, containing all the configurations that have been applied during the Terraform workflow&lt;/li&gt;
&lt;li&gt;By default, it is named as &lt;code&gt;terraform.tfstate&lt;/code&gt; and stored in the current/same directory as your configuration files.&lt;/li&gt;
&lt;li&gt;If working in a team, it&amp;rsquo;s encouraged to store the Terraform state file remotely. To enable collaboration, Terraform allows remote storage of the state file using the concept of &lt;em&gt;&lt;strong&gt;state backends&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Conventionally, use &lt;code&gt;backend.tf&lt;/code&gt; for the filename with example content below:
&lt;ul&gt;
&lt;li&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-terraform&#34; data-lang=&#34;terraform&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;terraform&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#a6e22e&#34;&gt;backend&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gcs&amp;#34;&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;bucket&lt;/span&gt; = &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;lt;PROJECT_ID&amp;gt;-tf-state&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;prefix&lt;/span&gt; = &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;backend&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;prefix&lt;/code&gt; refers to a folder on the bucket that will host the state file&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;commands&#34;&gt;Commands&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;terraform state list&lt;/code&gt;: list all resources in the state file&lt;/li&gt;
&lt;li&gt;&lt;code&gt;terraform state show &amp;lt;resource_name&amp;gt;&lt;/code&gt;: show detailed information about a specific resource in the state file&lt;/li&gt;
&lt;li&gt;&lt;code&gt;terraform state rm &amp;lt;resource_name&amp;gt;&lt;/code&gt;: remove a specific resource from the state file without deleting the actual resource in the cloud provider&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you made some manual changes to the resource after deployment:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;terraform plan -refresh-only&lt;/code&gt;: to visualize the difference between the Terraform state and the real infra&lt;/li&gt;
&lt;li&gt;&lt;code&gt;terraform apply -refresh-only&lt;/code&gt;: to update the state file to match the real infra according to the prev plan output without making any changes to the actual resources on the cloud&lt;/li&gt;
&lt;li&gt;Update the TF configuration file to set the same value as the manually modified value. So, if TF commands are reapplied, it won&amp;rsquo;t override the manually entered values.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;meta-arguments&#34;&gt;Meta-arguments&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Meta-arguments&lt;/strong&gt;&lt;/em&gt; are special constructs in Terraform to enable you to write efficient Terraform code. Examples of Meta-arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;provider&lt;/strong&gt;&lt;/em&gt; meta-argument&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can write as many providers as you want in the &lt;code&gt;provider.tf&lt;/code&gt; file. Try to keep only relevant providers in the same provider.tf file
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/tf_provider_eg.png&#34;
           alt=&#34;Terraform Provider Meta Argument&#34; width=&#34;80%&#34;/&gt;
  &lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;count&lt;/strong&gt;&lt;/em&gt; meta-argument: provides the capability to create identical resources in a single block.
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/tf_count_eg.png&#34;
         alt=&#34;Terraform Count Meta Argument&#34; width=&#34;45%&#34;/&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;for_each&lt;/strong&gt;&lt;/em&gt; meta-argument: If you want to create resources that share some attributes but have different attributes (e.g., worker nodes on different regions/zones), you can use for_each to map list of values as resource parameters.
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/tf_foreach_eg.png&#34;
         alt=&#34;Terraform for_each Meta Argument&#34; width=&#34;90%&#34;/&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;depends_on&lt;/strong&gt;&lt;/em&gt; meta-argument: By default, Terraform runs 10 resource operations in parallel. If you have a resource that can’t be created until another resource is created (e.g., instance using a specific subnet), then use depends_on meta-argument.
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/tf_dependson_eg.png&#34;
         alt=&#34;Terraform depends_on Meta Argument&#34; width=&#34;55%&#34;/&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;resource-reference-this&#34;&gt;Resource Reference &amp;ldquo;this&amp;rdquo;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In GCP, every resource can be uniquely identified, so it is good practice to refer to resources using &lt;code&gt;**this**&lt;/code&gt; reference, which is a &lt;em&gt;&lt;strong&gt;Unique IDentifier(UID)&lt;/strong&gt;&lt;/em&gt; containing additional information about the resource.&lt;/li&gt;
&lt;li&gt;You can use &lt;code&gt;this&lt;/code&gt; with any resource and refer to it in other resources. It provides a clear and widely understood convention for identifying the primary resource within a module.&lt;/li&gt;
&lt;li&gt;When a module is designed to be generic and reusable, &amp;ldquo;this&amp;rdquo; can provide a consistent naming scheme across different implementations.&lt;/li&gt;
&lt;/ul&gt;
- http://allenhsm.github.io/tech/cloud_infra_basics/terraform/ - This is a customized copyright.</description>
        </item>
    
    
    
        <item>
        <title>Apache Kafka</title>
        <link>http://allenhsm.github.io/tech/cloud_infra_basics/apache_kafka/</link>
        <pubDate>Tue, 25 Nov 2025 22:10:31 -0500</pubDate>
        
        <guid>http://allenhsm.github.io/tech/cloud_infra_basics/apache_kafka/</guid>
        <description>Allen&#39;s Blog http://allenhsm.github.io/tech/cloud_infra_basics/apache_kafka/ -&lt;h1 id=&#34;why-kafka&#34;&gt;Why Kafka?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Problem of Request-Response Architecture:&lt;/strong&gt; Kubernetes offers the ability to (de)scale your applications to respond to user requests in a meaningful amount of time.
&lt;ul&gt;
&lt;li&gt;Process of request-response architecture:
&lt;ul&gt;
&lt;li&gt;user sends a request to your application&lt;/li&gt;
&lt;li&gt;k8s master reroutes the request to an available pod,&lt;/li&gt;
&lt;li&gt;not enough pods&lt;/li&gt;
&lt;li&gt;HPA&lt;/li&gt;
&lt;li&gt;Still not enough&lt;/li&gt;
&lt;li&gt;VPA&lt;/li&gt;
&lt;li&gt;more requests&lt;/li&gt;
&lt;li&gt;Connection timeout&lt;/li&gt;
&lt;li&gt;Interruption&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If the number of user requests continue to increase beyond your K8s cluster resources, 1) your k8s master won&amp;rsquo;t have enough memory or processing power to reroute the incoming traffic, or 2) there are no available pods to process requests, and no more pods can be created.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Solution: message queues:&lt;/strong&gt; Kafka acts as a buffer between user requests and your application to avoid message drop &amp;amp; timeout problems. It is a &lt;strong&gt;scalable&lt;/strong&gt; message queue with &lt;strong&gt;publish-subscribe&lt;/strong&gt; software support that offers &lt;strong&gt;distributed&lt;/strong&gt; &lt;strong&gt;streaming processing&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;digression-1-publish-subscribe-model&#34;&gt;Digression 1: Publish-Subscribe Model&lt;/h2&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/publish_subscribe.png&#34;
         alt=&#34;publish-subscribe models&#34; width=&#34;95%&#34;/&gt;
&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;Model 1 → &lt;strong&gt;Kafka&lt;/strong&gt;: Sub A and B get the same message from the topic they subscribed to. They get the full copy by default. (topic-based pub-sub model)&lt;/li&gt;
&lt;li&gt;Model 4 → &lt;strong&gt;Active MQ&lt;/strong&gt;: Load balancing, traffic distribution, such that if messages A and B are sent, then Worker A only gets A, worker B only gets B. (queue-based/broker-mediated model)&lt;/li&gt;
&lt;li&gt;Model 2 → &lt;strong&gt;JMS, AWS Event Bridge&lt;/strong&gt;: Subscribers find the condition / set the filters in the queue such that the subs define the rules on how they get the data. The queue is like a pool, does not categorize the messages into categories. Broker applies the subscribers’ rules on every msg. (content-based pub-sub model)&lt;/li&gt;
&lt;li&gt;Model 3 → &lt;strong&gt;Kafka combined with KSQL&lt;/strong&gt;: Hybrid version combining 1 and 2. (topic+content)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ranking&#34;&gt;Ranking&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Scalability: Best: &lt;strong&gt;Topic based&lt;/strong&gt;, worst: queue based&lt;/li&gt;
&lt;li&gt;Speed: Best: &lt;strong&gt;topic based&lt;/strong&gt;, worst: content-based&lt;/li&gt;
&lt;li&gt;Flexibility: Best: content-based, worst: queue-based&lt;/li&gt;
&lt;li&gt;Therefore, topic based model is the most convenient&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pub-sub-model-example&#34;&gt;Pub-sub model example&lt;/h3&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/pub_sub_model.png&#34;
         alt=&#34;pub-sub model example&#34; width=&#34;90%&#34;/&gt;
&lt;/figure&gt;

&lt;h1 id=&#34;apache-kafka&#34;&gt;Apache Kafka&lt;/h1&gt;
&lt;h2 id=&#34;advantages&#34;&gt;Advantages&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;High throughput: can handle millions of messages per second (LinkedIn 3.2Million msgs/sec)&lt;/li&gt;
&lt;li&gt;High performance: uses in-memory writes and reads&lt;/li&gt;
&lt;li&gt;Fault tolerant: Kafka is highly available and resilient to node failures and supports automatic recovery.&lt;/li&gt;
&lt;li&gt;Elastically scalable, low operational overhead, durable, highly available&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;structure&#34;&gt;Structure&lt;/h2&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/kafka_structure.png&#34;
         alt=&#34;kafka structure&#34; width=&#34;70%&#34;/&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;topics&#34;&gt;Topics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Is a particular stream of data. Is the &lt;strong&gt;unit of parallelism&lt;/strong&gt; in Apache Kafka.&lt;/li&gt;
&lt;li&gt;You can have as many topics as you want&lt;/li&gt;
&lt;li&gt;Identified by a name, divided into partitions&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;partitions&#34;&gt;Partitions&lt;/h3&gt;
&lt;p&gt;To enhance scalability, we create partitions to the topics, each piece sits in independent space in the memory, and we can have as many partitions as we want for the same topic. But if the producer write  messages into the same partition, we cannot guarantee the consumer to do the changes in the same order. So within the same partition, the messages are ordered through offset, but there is no order across partitions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each partition is ordered&lt;/li&gt;
&lt;li&gt;Each message within a partition has an &lt;code&gt;offset&lt;/code&gt; (an incremental, unique id)&lt;/li&gt;
&lt;li&gt;To enable fault tolerance, we can get &lt;strong&gt;duplication of partition&lt;/strong&gt; in indifferent brokers.&lt;/li&gt;
&lt;li&gt;Each partition lives in an individual machine, called broker&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;brokers&#34;&gt;Brokers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A Kafka cluster is made up of multiple &lt;code&gt;brokers&lt;/code&gt; (servers)&lt;/li&gt;
&lt;li&gt;Each broker is identified by an id (integer)&lt;/li&gt;
&lt;li&gt;Each broker contains one or more partitions for one or more topics&lt;/li&gt;
&lt;li&gt;After connecting to any broker, you will be connected to the entire cluster (If i have the addr of B1, I also got the addr of B2, B3, etc)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;producers&#34;&gt;Producers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Producers publish/write data to topics (which are made of partitions) while does not know which partition it will write to. Broker will deal with that&lt;/li&gt;
&lt;li&gt;Producers automatically know to which broker and partition they should write to&lt;/li&gt;
&lt;li&gt;In case of Broker failure, producers will automatically recover the failed broker&lt;/li&gt;
&lt;li&gt;Producers choose whether to receive an &lt;strong&gt;acknowledgment&lt;/strong&gt; of a successful write (&lt;code&gt;ack&lt;/code&gt;), and whether to send a &lt;strong&gt;key&lt;/strong&gt; with the message. If two msgs are sent with the same key, we can guarantee they will be sent to the same partition, even though we don&amp;rsquo;t know which partition it is.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;consumers&#34;&gt;Consumers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Consumers read data from a topic (identified by a topic name)&lt;/li&gt;
&lt;li&gt;Consumers know which broker to read from.&lt;/li&gt;
&lt;li&gt;In case of Broker failures, consumers know how to recover the failed broker.&lt;/li&gt;
&lt;li&gt;Data are read in order within each partition.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Consumer group&lt;/strong&gt;: a set of consumers that share and load-balance the partitions of a topic, ensuring each partition is consumed by &lt;strong&gt;exactly one&lt;/strong&gt; consumer in the group. Different groups consume &lt;strong&gt;independently&lt;/strong&gt;, so each group receives a full copy of the topic’s messages.&lt;/li&gt;
&lt;li&gt;Why &lt;code&gt;group.id&lt;/code&gt; helps:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Parallelism&lt;/strong&gt;: all consumers with the same group.id share the work of reading a topic, Each partition’s messages go to exactly one consumer in the group. Add more consumers to the group → partitions get redistributed → higher parallelism (up to number of partitions).&lt;/li&gt;
&lt;li&gt;At-least-once delivery/&lt;strong&gt;fault tolerance&lt;/strong&gt;: each consumer in the group keeps track of its read offsets. If a consumer crashes and restarts, it resumes reading from its last committed offset, ensuring no messages are missed.&lt;/li&gt;
&lt;li&gt;Multiple independent applications: Different groups consume &lt;strong&gt;independently&lt;/strong&gt;, so each group receives a full copy of the topic’s messages.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;zookeeper&#34;&gt;Zookeeper&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Connects Partition and OS adding ports or system calls to OS such that partition can interact with the memory, to write and read the memory.&lt;/li&gt;
&lt;li&gt;Manages and coordinates the brokers (keeps a list of them)&lt;/li&gt;
&lt;li&gt;Helps in performing election for partitions&lt;/li&gt;
&lt;li&gt;Sends notification to Kafka in case of changes (new broker added, broker failure, new topic, etc)&lt;/li&gt;
&lt;li&gt;An important component of Kafka.&lt;/li&gt;
&lt;li&gt;Causes the hardness of deploying Kafka as it is on the top of OS to build the communication, and enable the Kafka to run smoothly and directly in different platforms and OS.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;parallelism&#34;&gt;Parallelism&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A topic partition is the unit of parallelism in Kafka. Each partition can be hosted on a different broker (server) in the Kafka cluster.&lt;/li&gt;
&lt;li&gt;Producers and brokers:
&lt;ul&gt;
&lt;li&gt;Writes to different partitions can be done in parallel.&lt;/li&gt;
&lt;li&gt;Parallelism frees up hardware resources for operations like compression.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Consumers:
&lt;ul&gt;
&lt;li&gt;You can have up to one consumer instance per partition within a consumer group.&lt;/li&gt;
&lt;li&gt;Any additional consumers beyond the number of partitions will remain idle.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Kafka cluster: more partitions in a Kafka cluster lead to higher system throughput.&lt;/li&gt;
&lt;li&gt;Scalable partitioning: you can increase or decrease the number of partitions in a topic as your data volume and throughput requirements change over time.&lt;/li&gt;
&lt;li&gt;Keyed messages caution: exercise caution when messages are produced with keys, as they are deterministically assigned to partitions.&lt;/li&gt;
&lt;li&gt;Consistent routing: kafka ensures that messages with identical keys are consistently routed to the same partition, preserving the integrity of order-dependent applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;digression-2-microservices&#34;&gt;Digression 2: Microservices&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Divide a computation into small, mostly stateless components that can be: easily replicated for scale, communicate with simple protocols, computation is as a swarm of communicating workers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stateless&lt;/strong&gt;: each component does not store any state between requests. Each request is independent.&lt;/li&gt;
&lt;li&gt;In cloud, it typically runs as containers using a service deployment and management service on systems like: Amazon Elastic Container service, GKE, DCOS from Berkeley/Mesosphere, Docker Swarm&lt;/li&gt;
&lt;li&gt;Reading: Example of Ecommerce microservice app deployed on GKE: &lt;a href=&#34;https://github.com/GoogleCloudPlatform/terraform-ecommerce-microservices-on-gke&#34;&gt;https://github.com/GoogleCloudPlatform/terraform-ecommerce-microservices-on-gke&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;digression-3-rest&#34;&gt;Digression 3: REST&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;provide an interface for distributed hypermedia systems, created by Roy Fielding in 2000&lt;/li&gt;
&lt;li&gt;The key abstraction of information in REST is a resource
&lt;ul&gt;
&lt;li&gt;Identified by a resource identifier, i.e. URI&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Resources can be retrieved or transformed to another state by aset of methods:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GET/PUT(update) /POST (add)&lt;/strong&gt;/DELETE/PATCH (Update a lot of resources)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The clients and servers exchange representations of resources by using a standardized interface and protocol typically HTTP&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kafka--mircroservices&#34;&gt;Kafka + Mircroservices&lt;/h2&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/kafka_microservices.png&#34;
         alt=&#34;kafka microservices&#34; width=&#34;100%&#34;/&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;code-example&#34;&gt;Code Example&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Producer example (Python):
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; socket
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; confluent_kafka &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Producer
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Kafka settings&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;BROKER &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;localhost:9092&amp;#39;&lt;/span&gt;   &lt;span style=&#34;color:#75715e&#34;&gt;# Change this to your Kafka broker address&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;TOPIC &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;simple_topic&amp;#39;&lt;/span&gt;      &lt;span style=&#34;color:#75715e&#34;&gt;# Replace with your Kafka topic&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Function to create a Kafka Producer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;create_kafka_producer&lt;/span&gt;(broker):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    conf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;bootstrap.servers&amp;#39;&lt;/span&gt;: broker,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;client.id&amp;#39;&lt;/span&gt;: socket&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;gethostname()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    producer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Producer(conf)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; producer
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;publish_simple_message&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    producer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; create_kafka_producer(BROKER)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    producer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;produce(TOPIC, key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;message&amp;#34;&lt;/span&gt;, value&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Yes, it&amp;#39;s me!&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    producer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;flush()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; __name__ &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;__main__&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    publish_simple_message()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;Consumer example - creation code:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Creation code&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; socket
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; confluent_kafka &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Producer
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Kafka settings&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;BROKER &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;localhost:9092&amp;#39;&lt;/span&gt;   &lt;span style=&#34;color:#75715e&#34;&gt;# Change this to your Kafka broker address&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;TOPIC &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;simple_topic&amp;#39;&lt;/span&gt;      &lt;span style=&#34;color:#75715e&#34;&gt;# Replace with your Kafka topic&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Function to create a Kafka Producer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;create_kafka_producer&lt;/span&gt;(broker):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    conf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;bootstrap.servers&amp;#39;&lt;/span&gt;: broker,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;client.id&amp;#39;&lt;/span&gt;: socket&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;gethostname()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    producer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Producer(conf)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; producer
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;publish_simple_message&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    producer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; create_kafka_producer(BROKER)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    producer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;produce(TOPIC, key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;message&amp;#34;&lt;/span&gt;, value&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Yes, it&amp;#39;s me!&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    producer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;flush()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; __name__ &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;__main__&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    publish_simple_message()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;Consumer example: poll loop
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Display data from Kafka&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;display_kafka_data&lt;/span&gt;():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    consumer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; create_kafka_consumer(BROKER, GROUP_ID, TOPIC)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        msg &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; consumer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;poll(timeout&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; msg &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; msg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;error():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; msg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;error()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;code() &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; KafkaError&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_PARTITION_EOF:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                print(msg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;error())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; msg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;key()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;decode(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        value &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; msg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;value()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;decode(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Display message with an icon&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;New Message Alert!!&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Key: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;key&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; and Value: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;value&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    consumer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;close()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; __name__ &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;__main__&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    display_kafka_data()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;confluent-kafka&#34;&gt;Confluent Kafka&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Since Handling Kafka requires managing low-level, complex data infrastructure, we have &lt;strong&gt;Confluent Kafka&lt;/strong&gt;. It is built on top of Apache Kafka to offer complete, fully managed, cloud-native data streaming that&amp;rsquo;s available wherever your data and applications reside.&lt;/li&gt;
&lt;/ul&gt;
- http://allenhsm.github.io/tech/cloud_infra_basics/apache_kafka/ - This is a customized copyright.</description>
        </item>
    
    
    
        <item>
        <title>Deployment Orchestration</title>
        <link>http://allenhsm.github.io/tech/cloud_infra_basics/deployment_orchestration/</link>
        <pubDate>Sun, 23 Nov 2025 21:01:28 -0500</pubDate>
        
        <guid>http://allenhsm.github.io/tech/cloud_infra_basics/deployment_orchestration/</guid>
        <description>Allen&#39;s Blog http://allenhsm.github.io/tech/cloud_infra_basics/deployment_orchestration/ -&lt;h1 id=&#34;container-orchestration&#34;&gt;Container Orchestration&lt;/h1&gt;
&lt;p&gt;The &lt;strong&gt;automatic&lt;/strong&gt; process of managing or scheduling the work of individual containers for applications based on &lt;strong&gt;microservices&lt;/strong&gt; within &lt;strong&gt;multiple clusters&lt;/strong&gt;.
&lt;strong&gt;Orchestration tools&lt;/strong&gt;: Kubernetes, Docker Swarm, Apache Mesos, CoreOS rkt&lt;/p&gt;
&lt;h1 id=&#34;kubernetes&#34;&gt;Kubernetes&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;An open-source container management tool which automates container deployment, container (de)scaling, container load balancing&lt;/li&gt;
&lt;li&gt;Can group &amp;ldquo;n&amp;rdquo; number of containers into one logical unit called &amp;ldquo;&lt;strong&gt;POD&lt;/strong&gt;&amp;rdquo;, easy to manage and deploy&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Automated bin packing&lt;/strong&gt;: via packaging software and automatically placing it/containers based on their resource requirements and other constraints, while not sacrificing availability. Mix critical and best-effort workloads in order to drive up utilization and save more resources.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service Discovery and Load Balancing&lt;/strong&gt;: Using auto networking and load balancing configurations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage Orchestration&lt;/strong&gt;: Automatically mount the storage system of your choice for the cluster, whether from local storage, a public cloud provider, or a network storage system such as NFS, iSCSI, GlusterFS.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Self Healing&lt;/strong&gt;: Automatically restarts containers that fail, replaces and reschedules containers when nodes die, kills containers that don&amp;rsquo;t respond to your user-defined health check, and doesn&amp;rsquo;t advertise them to clients until they are ready to serve.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Automated Rollouts and Rollbacks&lt;/strong&gt;: Progressively roll out changes to your application or its configuration, while monitoring application health to ensure it doesn&amp;rsquo;t kill all your instances at the same time. Making sure an update or rollback will not disrupt the ongoing traffic.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Secret and Configuration Management&lt;/strong&gt;: dDploy and update secrets and application configuration without rebuilding your image and without exposing secrets in your stack configuration.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Batch Execution&lt;/strong&gt;: In addition to services, Kubernetes can manage your batch and CI workloads.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Horizontal Scaling&lt;/strong&gt;: Scale your application up and down with a simple command, a UI, or automatically scaling based on CPU usage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Infrastructure as Code&lt;/strong&gt;: Will discuss in later articles.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;k8s-architecture&#34;&gt;&lt;strong&gt;K8s Architecture&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/k8s_architecture.png&#34;
         alt=&#34;k8s architecture&#34; width=&#34;40%&#34;/&gt;
&lt;/figure&gt;

&lt;strong&gt;Simple version&lt;/strong&gt;: Master -&amp;gt; Nodes -&amp;gt; PODs -&amp;gt; Containers&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Master&lt;/strong&gt; controls the clusters and the nodes in it&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nodes&lt;/strong&gt; host the group of containers called &lt;strong&gt;POD&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Containers&lt;/strong&gt; in a &lt;code&gt;POD&lt;/code&gt; run on the same node and share resources such as filesystems, kernel namespaces, and an IP address.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Replication Controller&lt;/strong&gt; at the &lt;code&gt;Master&lt;/code&gt; ensure that requested number of PODs are running on nodes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Load Balancer&lt;/strong&gt; at the &lt;code&gt;Master&lt;/code&gt; provide load balancing across a replicated group of PODs&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;master-and-worker-node&#34;&gt;&lt;strong&gt;Master and Worker Node&lt;/strong&gt;&lt;/h3&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/master_worker_node.png&#34; width=&#34;50%&#34;/&gt;
&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Kubeproxy&lt;/strong&gt;: is used for configuring Kubernetes Proxy Server.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kubelet&lt;/strong&gt;: is the primary &amp;ldquo;node agent&amp;rdquo; that runs on each node. It can register the node with the Master Node using node specific information.&lt;strong&gt;Controller vs. Scheduler&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Controller&lt;/strong&gt;: responsible for regulating the &lt;strong&gt;state&lt;/strong&gt; of the system. Used by master to ensure the actual state of the system matches the desired state as specified in Kubernetes resource configurations (e.g., Deployments, StatefulSets). Nodes also can access &lt;code&gt;deployment.yaml&lt;/code&gt; to know the desired state.
&lt;ul&gt;
&lt;li&gt;Monitors the state of the cluster and takes action to &lt;strong&gt;reconcile&lt;/strong&gt; the difference between the desired and actual states.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scheduler&lt;/strong&gt;: determines where new Pods should be placed (i.e., scheduled) on the cluster nodes.
&lt;ul&gt;
&lt;li&gt;When a Pod is created but not assigned to a specific node, the Scheduler evaluates available nodes and selects the most suitable one based on resource requirements, node health, other constraints, and policies.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kubernetes-service&#34;&gt;&lt;strong&gt;Kubernetes Service&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;An abstraction which defines a &lt;strong&gt;logical set of Pods&lt;/strong&gt; running somewhere in your cluster that all provide the same functionality. and a policy by which to access them. The set of Pods targeted by a Service is usually determined by a &lt;strong&gt;label selector&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When created, each &lt;code&gt;Service&lt;/code&gt; is assigned a unique IP address (&lt;code&gt;ClusterIP&lt;/code&gt;) and a DNS name, which remains constant as the underlying Pods change.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Cluster IP&lt;/code&gt; addr is tied to the lifespan of the &lt;code&gt;Service&lt;/code&gt; and will not change while the &lt;code&gt;Service&lt;/code&gt; is alive.&lt;/li&gt;
&lt;li&gt;Pods can be configured to talk to the Service and know that communication to the Service will be automatically load-balanced out to some pod that is a member of the Service.&lt;/li&gt;
&lt;li&gt;All the service-related configurations are specified in &lt;code&gt;services.yaml&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Types of service&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ClusterIP&lt;/strong&gt;: Exposes the service(pod) on a cluster-internal IP. This is the default &lt;code&gt;ServiceType&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NodePort&lt;/strong&gt;: Exposes the service on each Node’s IP at a static port (the &lt;code&gt;NodePort&lt;/code&gt;) to external traffic by fowarding traffic from the port on each node of the cluster to the container port. A &lt;code&gt;ClusterIP&lt;/code&gt; service, to which the &lt;code&gt;NodePort&lt;/code&gt; service routes, is automatically created. It is &lt;strong&gt;cheaper&lt;/strong&gt;, because has no actual traffic control.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LoadBalancer&lt;/strong&gt;: Also exposes the service externally, but using an extra load balancer from cloud provider. It does not have an exposed physical port. &lt;code&gt;NodePort&lt;/code&gt; and &lt;code&gt;ClusterIP&lt;/code&gt; services, to which the external load balancer routes, are automatically created. (It is more &lt;strong&gt;expensive&lt;/strong&gt;, because of traffic distribution management(&lt;strong&gt;TDM&lt;/strong&gt;))&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deployment--service&#34;&gt;&lt;strong&gt;Deployment + Service&lt;/strong&gt;&lt;/h3&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/deployment_and_service.png&#34;
         alt=&#34;deployment and service&#34; width=&#34;40%&#34;/&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;commands-on-gke&#34;&gt;Commands on GKE&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Create a GKE cluster:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;gcloud container clusters create &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  —machine-type e2-standard-2 &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  —num-nodes 2&lt;span style=&#34;color:#ae81ff&#34;&gt;\ &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  —zone us-central1-a &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  —cluster-version latest &lt;span style=&#34;color:#ae81ff&#34;&gt;\ &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  mykubernetescluster
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kubectl get nodes&lt;/code&gt; : list all nodes in the cluster&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kubectl get pods&lt;/code&gt; : list all pods in the cluster&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kubectl apply -f deployment.yaml&lt;/code&gt; : create deployment from &lt;code&gt;deployment.yaml&lt;/code&gt; file. It also creates the pods defined in the deployment file.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kubectl get deployments&lt;/code&gt; : list all deployments in the cluster&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kubectl apply -f service.yaml&lt;/code&gt; : create service to expose app to external traffic.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kubectl scale deployment &amp;lt;deployment-name&amp;gt; --replicas=&amp;lt;number-of-replicas&amp;gt;&lt;/code&gt; : scale the number of replications in a deployment refer to the number of pods in a deployment&lt;/li&gt;
&lt;li&gt;auto-scaling:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;kubectl autoscale deployment &amp;lt;deployment-name&amp;gt; --min=&amp;lt;min-replicas&amp;gt; --max=&amp;lt;max-replicas&amp;gt; --cpu-percent=&amp;lt;target-cpu-utilization-percentage&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;e.g. &lt;code&gt;kubectl autoscale deployment nginx-deployment --min=2 --max=10 --cpu-percent=80&lt;/code&gt; where cpu  percent is the average cpu usage across all the pods. here it is using HPA(horizontal pod autoscaler)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kubectl get hpa&lt;/code&gt; : list all the HPA in the cluster&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kubectl delete hpa &amp;lt;deployment-name&amp;gt;&lt;/code&gt; : delete the HPA for a deployment&lt;/li&gt;
&lt;li&gt;2 types of auto-scaling:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Horizontal Pod Autoscaler&lt;/strong&gt; (HPA): scale the number of pods &lt;em&gt;replicas&lt;/em&gt; up or down in a deployment based on cpu/memory usage or other custom metrics. It modifies the &lt;strong&gt;number of&lt;/strong&gt; pods to balance load.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vertical Pod Autoscaler&lt;/strong&gt; (VPA): automatically adjust the cpu and memory requests/limits for the containers in the pods, so it is kind of changing the &lt;strong&gt;size of&lt;/strong&gt; pods, without changing the number of pods.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;To debug K8s application: &lt;a href=&#34;https://kubernetes.io/docs/tasks/debug/debug-application/debug-running-pod/&#34;&gt;https://kubernetes.io/docs/tasks/debug/debug-application/debug-running-pod/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;yaml&#34;&gt;YAML&lt;/h1&gt;
&lt;h2 id=&#34;basics&#34;&gt;Basics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;YAML (YAML Ain&amp;rsquo;t Markup Language) is a human-readable data serialization standard that can be used in conjunction with all programming languages and is often used to write configuration files.&lt;/li&gt;
&lt;li&gt;There are two main structures in YAML: &lt;strong&gt;Maps&lt;/strong&gt; (key-value pairs) and &lt;strong&gt;Lists&lt;/strong&gt; (ordered lists).
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Maps&lt;/strong&gt;: You can use map when you are trying to assign a scalar value
&lt;ul&gt;
&lt;li&gt;Example:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;John Doe&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;age&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v2.4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pod&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lists&lt;/strong&gt;: sequences of objects that are treated as a single unit with the same key
&lt;ul&gt;
&lt;li&gt;Example:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;webserver1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx:latest&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;: 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;database-server&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql:5.7&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3306&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;create-pods&#34;&gt;Create Pods&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;code&gt;Pod&lt;/code&gt; is the smallest and most basic deployable object in K8s. A Pod represents a &lt;strong&gt;single instance of a running process&lt;/strong&gt; in your cluster.&lt;/li&gt;
&lt;li&gt;Each pod can obtain &lt;strong&gt;one or more containers&lt;/strong&gt;, such as Docker Containers.&lt;/li&gt;
&lt;li&gt;When a Pod runs multiple containers, the containers are managed as a single entity and share the Pod&amp;rsquo;s resources. Generally, running multiple containers in a single Pod is an advanced use case.&lt;/li&gt;
&lt;li&gt;Pods also contain shared networking and storage resources for their containers:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Network&lt;/strong&gt;: Pods are automatically assigned unique IP addresses. Pod containers share the same network namespace, including IP address and network ports. Containers in a Pod communicate with each other inside the Pod on localhost.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: Pods can specify a set of shared storage volumes that can be shared among the containers.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;You can consider a Pod to be a self-contained unit, isolated &amp;ldquo;logical host&amp;rdquo; that contains the systemic needs of the application it servers.&lt;/li&gt;
&lt;li&gt;A Pod is meant to run a single instance of your application on your cluster. However, it is not recommended to create individual Pods directly. Instead, you generally create a set of identical Pods, called &lt;code&gt;replicas&lt;/code&gt;, to run your application. Such a set of replicated Pods are created and managed by a &lt;code&gt;Deployment&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;deployment&#34;&gt;Deployment&lt;/h2&gt;
&lt;p&gt;A deployment is a file that defines a pod&amp;rsquo;s desired behavior or characteristics. You create it to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ensure workload &lt;strong&gt;availability&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scale&lt;/strong&gt; workload&lt;/li&gt;
&lt;li&gt;Deployments can be paused, edited and rolled-back&lt;/li&gt;
&lt;li&gt;Simplify the interface with external network&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example code:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx-deployment&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# Deployment name&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;: &lt;span style=&#34;color:#75715e&#34;&gt;# defines how the created ReplicaSet &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# finds which Pods to manage&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx  &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;: &lt;span style=&#34;color:#75715e&#34;&gt;# We will run one Pod&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# containing one container&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx:1.14.2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To create it, we run: &lt;code&gt;kubectl apply -f deployment.yaml&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;service&#34;&gt;Service&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Motivation&lt;/strong&gt;: We need a way of consistently being able to access our pod-set even if the pods themselves are recreated or lost. This should be transparent to any application or other pods trying to access this pod-set.
&lt;ul&gt;
&lt;li&gt;Example: assume we have a Kubernetes deployment that contains web application and a database. When a user searches from our website’s frontend, the search request is directed to a set of pods where our database is defined. If the database pod goes down, Kubernetes master will recreate it, and the frontend will not be aware of this. The search request should still be acknowledged and fulfilled as usual.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Definition&lt;/strong&gt;: an abstract way to expose an application running on a set of Pods as a network service.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example code&lt;/strong&gt;:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx-service&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# Service name&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NodePort&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# Service type&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# a selector that the Service uses &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# to match and identify the pods in &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Nginx Deployment, because the deployment &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# and the pods also have the exact same label.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# This is how all incoming requests to this Nginx service &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# will be automatically routed to the Nginx deployment&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;: &lt;span style=&#34;color:#75715e&#34;&gt;# Using this NodePort, will be able to access the Nginx&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# service on all K8s nodes via port 30500&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;nodePort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;30500&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# points to our Nginx deployment&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;To create this service, we run: &lt;code&gt;kubectl apply -f service.yaml&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;reading&#34;&gt;Reading&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes basics first 6 chapters: &lt;a href=&#34;https://kubernetes.io/docs/tutorials/kubernetes-basics/&#34;&gt;https://kubernetes.io/docs/tutorials/kubernetes-basics/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deploy a Docker containerized web app to GKE: &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app&#34;&gt;https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
- http://allenhsm.github.io/tech/cloud_infra_basics/deployment_orchestration/ - This is a customized copyright.</description>
        </item>
    
    
    
        <item>
        <title>Containerization</title>
        <link>http://allenhsm.github.io/tech/cloud_infra_basics/containerization/</link>
        <pubDate>Sat, 22 Nov 2025 22:25:09 -0500</pubDate>
        
        <guid>http://allenhsm.github.io/tech/cloud_infra_basics/containerization/</guid>
        <description>Allen&#39;s Blog http://allenhsm.github.io/tech/cloud_infra_basics/containerization/ -&lt;h1 id=&#34;virtual-machines-to-containers&#34;&gt;Virtual Machines to Containers&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;VM is too &lt;strong&gt;heavy&lt;/strong&gt; for a simple process as it boots a whole OS every time.&lt;/li&gt;
&lt;li&gt;Containers are &lt;strong&gt;isolated&lt;/strong&gt;, but share OS, and where appropriate, also bins/libs. So SDE focuses more on app. &lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/vm_to_container.png&#34; alt=&#34;VM to Container&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Comparison:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lightweight&lt;/strong&gt;: Containerization avoids duplication of app Bins/libs, and only save the modifications did by different containers, without changing the original binaries.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: VM is more secure because of full isolation while Container has only process level isolation.
&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/vm_vs_container.png&#34; alt=&#34;VM vs. Container&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;motivation-of-containerization&#34;&gt;Motivation of Containerization&lt;/h1&gt;
&lt;h2 id=&#34;development-challenges&#34;&gt;Development challenges:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Containerization makes version control much more easier, and more importantly, safer, to avoid a simple app influence the whole system.&lt;/li&gt;
&lt;li&gt;Solves the conflict of different dependencies, versions, environments(&lt;code&gt;N x M&lt;/code&gt; configurations， multiplicity of hardware env), multiplicity of stacks, and contents.&lt;/li&gt;
&lt;li&gt;Modularity and scalability: can be quickly and smoothly migrated &amp;amp; scaled, ensuring compatibility across different environments.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;solution-containerization&#34;&gt;Solution: Containerization&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/sol_by_containerization.png&#34; alt=&#34;solution by containerization&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;docker---leading-container&#34;&gt;Docker - leading container&lt;/h1&gt;
&lt;h2 id=&#34;why-popular&#34;&gt;Why popular&lt;/h2&gt;
&lt;p&gt;By mid-2013, &lt;code&gt;dotCloud&lt;/code&gt; company released a tool that used these ideas to provide a better way to deploy encapsulated applications. The tool later became Docker and &lt;code&gt;dotCloud&lt;/code&gt; became Docker Inc&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Most widely known, and used&lt;/li&gt;
&lt;li&gt;Easy to install and use&lt;/li&gt;
&lt;li&gt;Free&lt;/li&gt;
&lt;li&gt;Load fast: sharing library among containers&lt;/li&gt;
&lt;li&gt;Docker Hub: public registry of container images&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Its file system!&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;union-file-systemufs&#34;&gt;Union File System(UFS)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Union File Systems&lt;/strong&gt; basically allow you to take different file systems and create a union of their contents with the top-most layer superseding any similar files found in the file systems below it.
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/layers_of_image.png&#34;
           alt=&#34;layers of image&#34; width=&#34;30%&#34;/&gt;
  &lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;Docker images are composed of &lt;strong&gt;layers&lt;/strong&gt; in the UFS. The image is itself a stack of read-only directories. The base is a simplified Linux file system.
&lt;ul&gt;
&lt;li&gt;Additional tools that the container needs are then layered on top of this base image, each in its own layer. Each layer represents an instruction in the image’s Dockerfile.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Packages in the hub has &lt;strong&gt;Layers&lt;/strong&gt; which can be some personalized modification upon the original package/app. But the layers can be shared by different app/packages such that the next time you pull some packages sharing same layers, no need to download again.&lt;/li&gt;
&lt;li&gt;All containers with the same image see the same &lt;strong&gt;directory tree&lt;/strong&gt;, so it loads the directory tree in the memory only once among all instances&lt;/li&gt;
&lt;li&gt;When the container is run, a final &lt;strong&gt;writable file system&lt;/strong&gt; is layered on top.&lt;/li&gt;
&lt;li&gt;As an app in the container executes, if it needs to modify an object in the read-only layers, it copies those objects into the writable layer. Otherwise, it uses the data in the read-only layer, which is shared with other container instances.&lt;/li&gt;
&lt;li&gt;Thus, typically only a little of the container image needs to be actually loaded when a container is run, which means that containers can load and run much faster than virtual machines. Launching a container typically takes less than a second.&lt;/li&gt;
&lt;li&gt;In addition to the file system layers in the container image, you can mount a &lt;strong&gt;host machine directory&lt;/strong&gt; as a file system in the container’s OS. In this way, a container can share data with the host. Multiple containers can also share these &lt;strong&gt;mounted directories&lt;/strong&gt; and can use them for basic communication of shared data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;docker-dev-workflow&#34;&gt;Docker Dev Workflow&lt;/h2&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/docker_workflow.png&#34;
           alt=&#34;docker workflow&#34; width=&#34;50%&#34;/&gt;
  &lt;/figure&gt;

&lt;h3 id=&#34;docker-lifecycle-overview&#34;&gt;Docker Lifecycle Overview&lt;/h3&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;figure class=&#34;img-center&#34;&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/docker_lifecycle.png&#34;
         alt=&#34;docker lifecycle&#34; width=&#34;60%&#34;/&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;dockerfile&#34;&gt;Dockerfile&lt;/h3&gt;
&lt;p&gt;Most important commands:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Instruction&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ADD&lt;/td&gt;
&lt;td&gt;Add local or remote files and directories.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ARG&lt;/td&gt;
&lt;td&gt;Use build-time variables.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CMD&lt;/td&gt;
&lt;td&gt;Specify default commands.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;COPY&lt;/td&gt;
&lt;td&gt;Copy files and directories.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ENTRYPOINT&lt;/td&gt;
&lt;td&gt;Specify default executable.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ENV&lt;/td&gt;
&lt;td&gt;Set environment variables.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;EXPOSE&lt;/td&gt;
&lt;td&gt;Describe which ports your application is listening on.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FROM&lt;/td&gt;
&lt;td&gt;Create a new build stage from a base image.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HEALTHCHECK&lt;/td&gt;
&lt;td&gt;Check a container&amp;rsquo;s health on startup.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LABEL&lt;/td&gt;
&lt;td&gt;Add metadata to an image.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MAINTAINER&lt;/td&gt;
&lt;td&gt;Specify the author of an image.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ONBUILD&lt;/td&gt;
&lt;td&gt;Specify instructions for when the image is used in a build.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RUN&lt;/td&gt;
&lt;td&gt;Execute build commands.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SHELL&lt;/td&gt;
&lt;td&gt;Set the default shell of an image.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;STOPSIGNAL&lt;/td&gt;
&lt;td&gt;Specify the system call signal for exiting a container.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;USER&lt;/td&gt;
&lt;td&gt;Set user and group ID.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VOLUME&lt;/td&gt;
&lt;td&gt;Create volume mounts.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;WORKDIR&lt;/td&gt;
&lt;td&gt;Change working directory.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;dockerfile-vs-docker-union-filesystem&#34;&gt;Dockerfile vs. Docker Union Filesystem&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/dockerfile_vs_ufs.png&#34; alt=&#34;dockerfile vs UFS&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;docker-commands&#34;&gt;Docker commands&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Build Your Docker Image (and tag it) – Notice the period at the end&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;docker build -t yourDockerHubID/your_image_name:tag .
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Example:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;docker buildx build --platform linux/amd64,linux/arm64 --push -t allenhu28/hw2-jupyter:latest .
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Test your image&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;docker run (-d) -p 8888:8888 yourDockerHubID/your_image_name:tag
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;or simply&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;docker run  8888:8888 yourDockerHubID/your_image_name:tag
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Upload to DockerHub&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;docker push yourDockerHubID/your_image_name:tag
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;More examples of build and add tag:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;docker build -t hw2-jupyter . 
docker tag hw2-jupyter allenhu28/hw2-jupyter:latest
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Docker Compose&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create and start all the containers listed in the “docker-compose.yml”: &lt;code&gt;docker-compose up -d&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;List all the containers belong to the compose environment instance: &lt;code&gt;docker-compose ps&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Set the number of containers: &lt;code&gt;docker-compose scale web=3&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multiple-containers&#34;&gt;Multiple containers&lt;/h3&gt;
&lt;p&gt;Solutions to connect several containers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Docker networking&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Docker Compose&lt;/strong&gt;: a tool for defining and running multi-container Docker applications. With Compose, you use a single &lt;strong&gt;YAML&lt;/strong&gt; file &lt;code&gt;docker-compose.yml&lt;/code&gt; to configure your multi-container application’s services. Then, with a single command, you create and start all the services from your configuration.
&lt;ul&gt;
&lt;li&gt;YML File example:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;version&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;3&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;services&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;web&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;build&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;8080:80&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;db&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;environment&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;MYSQL_ROOT_PASSWORD&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;example&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;3306:3306&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#ae81ff&#34;&gt;db_data:/var/lib/mysql&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;: &lt;span style=&#34;color:#75715e&#34;&gt;# Shared across services)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;db_data&lt;/span&gt;: {}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;Network&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;default&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;driver&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bridge&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;docker-limitations&#34;&gt;Docker Limitations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: Containers share the host OS kernel, so a malicious container could potentially exploit vulnerabilities in the kernel to compromise the host system or other containers.&lt;/li&gt;
&lt;li&gt;Does not offer &lt;strong&gt;cross-hardware-architecture portability&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Not easy to use with Desktop applications that &lt;strong&gt;require rich GUI&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Challenging to manage large number of docker containers &lt;strong&gt;manually&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
- http://allenhsm.github.io/tech/cloud_infra_basics/containerization/ - This is a customized copyright.</description>
        </item>
    
    
    
        <item>
        <title>Virtualization</title>
        <link>http://allenhsm.github.io/tech/cloud_infra_basics/virtualization/</link>
        <pubDate>Sat, 22 Nov 2025 21:01:07 -0500</pubDate>
        
        <guid>http://allenhsm.github.io/tech/cloud_infra_basics/virtualization/</guid>
        <description>Allen&#39;s Blog http://allenhsm.github.io/tech/cloud_infra_basics/virtualization/ -&lt;h1 id=&#34;virtualization&#34;&gt;Virtualization&lt;/h1&gt;
&lt;p&gt;to create independent execution environment, leveraging the abstract form of the hardware capability.&lt;/p&gt;
&lt;h2 id=&#34;more-defi-of-virtualization&#34;&gt;More Defi of Virtualization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;abstracts the hardware of computing infrastructure into several different execution environments.
&lt;ul&gt;
&lt;li&gt;Creates the illusion that each separate environment is running on its &lt;strong&gt;own private computing infrastructure&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Makes servers, workstations, storage, network, and other systems &lt;strong&gt;independent of the physical hardware&lt;/strong&gt; layer.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;It is the fundamental technology for cloud infrastructure.
&lt;ul&gt;
&lt;li&gt;Virtual resources can be started and stooped easily and quickly on demand.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;related-concepts&#34;&gt;Related concepts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;virtual machine (VM)&lt;/strong&gt;: an instance of a virtualized computing environment that behaves like a separate computer with its own OS, CPU, memory, storage, and network interfaces. A visual representation of a physical machine.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hypervisor&lt;/strong&gt; or &lt;strong&gt;Virtual machine monitor&lt;/strong&gt;: A process that separates a computer’s operating system and applications from the underlying physical hardware.
&lt;ul&gt;
&lt;li&gt;Hypervisor monitors and manages running virtual machines.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;host machine&lt;/strong&gt;: the physical machine on which the virtual machine runs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;guest machine&lt;/strong&gt;: the virtual machine that runs on the host machine.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;virtualization-types&#34;&gt;Virtualization types&lt;/h2&gt;
&lt;p&gt;The most important three types of virtualization:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Network: the process of combining hardware and software network resources and network functionality into a single, software-based administrative entity, a virtual network
&lt;ul&gt;
&lt;li&gt;Internal network virtualization: software switch, software router, software defined network(SDN)&lt;/li&gt;
&lt;li&gt;external network virtualization: VLAN, VXLAN&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Server&lt;/strong&gt;: Full, partial, para (most popular on cloud)&lt;/li&gt;
&lt;li&gt;Storage: block-level, file-level
&lt;ul&gt;
&lt;li&gt;pools physical storage from multiple network storage mediums to enable a single logical sotrage pool that is managed from a central console&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;virtualization-in-practice&#34;&gt;Virtualization in Practice&lt;/h2&gt;
&lt;h3 id=&#34;traditional-server-concept&#34;&gt;Traditional server concept&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Servers are viewed as an &lt;strong&gt;integral&lt;/strong&gt; computing unit that each unit includes the hardware, OS, storage, and related apps.&lt;/li&gt;
&lt;li&gt;Servers are often identified and referred to by their &lt;strong&gt;functions&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;File server, web server, database server, exchange server, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;When current server is overloaded, we need to add more physical servers to share the load.&lt;/li&gt;
&lt;li&gt;In the pic below, which is a traditional server infrastructure, server is the combination of hardware and software(软硬件互相绑定)
&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/traditional_server_failure.png&#34; alt=&#34;Traditional server failure&#34;&gt;&lt;/li&gt;
&lt;li&gt;Disadvantages:
&lt;ul&gt;
&lt;li&gt;Maintenance cost is high: acquisition and hardware repair cost&lt;/li&gt;
&lt;li&gt;Replication is challenging: redundancy is costly and difficult to implement&lt;/li&gt;
&lt;li&gt;Scalability is limited: physical resources are limited&lt;/li&gt;
&lt;li&gt;Highly vulnerable to hardware failure: single point of failure&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Often: Resource utilization is low&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;virtual-server-infrastructure&#34;&gt;Virtual server infrastructure&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Server sprawl&lt;/strong&gt; happens especially for on-promise and is the most threat for traditional server. To avoid this, we do virtualization.
The difference with traditional server infrastructure is that here the server is decoupled from the hardware. We add a new layer between {app, OS, storage} and hardware, which is the virtualization(&lt;strong&gt;hypervisor&lt;/strong&gt;) layer.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Server virtualization enable server Consolidation and Containment
&lt;ul&gt;
&lt;li&gt;Eliminating “&lt;strong&gt;server sprawl&lt;/strong&gt;” via deployment of systems as “virtual machines” that can run safely and move transparently across shared hardware&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A virtual server can be serviced by one or more hosts, and one host may house more than one virtual server.
&lt;ul&gt;
&lt;li&gt;This results in increased server utilization rates: from 5-15%, for traditional servers, to 60-80%&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Virtual servers can still be referred to by their &lt;strong&gt;function&lt;/strong&gt; i.e., email server, database server, etc.&lt;/li&gt;
&lt;li&gt;If the environment is built correctly, virtual servers will &lt;strong&gt;not be affected by the loss of a host&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Hosts may be removed and introduced almost at anytime to accommodate maintenance.&lt;/li&gt;
&lt;li&gt;Virtual servers can be scaled up and down easily.&lt;/li&gt;
&lt;li&gt;Server &lt;strong&gt;“cloning”&lt;/strong&gt; can be easily achieved
&lt;ul&gt;
&lt;li&gt;Multiple, identical virtual servers can be easily created based on server templates&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Virtual servers can be migrated from host to host dynamically, as needed.
&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/virtualization_structure.png&#34; alt=&#34;virtualization image&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;virtualization-advantages&#34;&gt;Virtualization advantages:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Resource optimization&lt;/strong&gt; that would result in reducing hardware, power and space requirement.&lt;/li&gt;
&lt;li&gt;Virtualization allows for the &lt;strong&gt;quick deployment, migration, and replication of VMs&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Support for &lt;strong&gt;Legacy Systems&lt;/strong&gt;: Virtualization allows legacy applications to run in a modern cloud environment without requiring significant changes to the underlying infrastructure&lt;/li&gt;
&lt;li&gt;Better &lt;strong&gt;automation&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hypervisors&#34;&gt;Hypervisors:&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;http://allenhsm.github.io/images/cloudInfraBasics/types_of_hypervisor.png&#34; alt=&#34;Two types of hypervisor&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Type 1 native hypervisor(bare-metal): runs directly on the host&amp;rsquo;s hardware to control the hardware and to manage guest operating systems. Examples: VMware ESXi, Microsoft Hyper-V, Xen&lt;/li&gt;
&lt;li&gt;Type 2 hosted hypervisor: runs as an application on a conventional operating system and supports virtual machines running as individual processes. Examples: VMware Workstation, Oracle VirtualBox, QEMU, JVM, UTM&lt;/li&gt;
&lt;/ul&gt;
- http://allenhsm.github.io/tech/cloud_infra_basics/virtualization/ - This is a customized copyright.</description>
        </item>
    
    
  </channel>
</rss> 